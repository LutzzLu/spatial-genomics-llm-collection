{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb165d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "# Configuration and API keys (replace placeholders with actual keys/emails)\n",
    "SEMANTIC_SCHOLAR_API_KEY = \"19tQFoyv7w5xBQNMsUA7C5lwNqEni5g3GKkP8Pkj\"  # e.g., 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' | https://www.semanticscholar.org/product/api/tutorial?utm_campaign=API%20transaction&utm_medium=email&_hsenc=p2ANqtz--KbD5dVfVRom22kVjKkL-55Ikb73h1Nze5JYW6_8OGfj15Pf_Z7OjRXzHnO2BntuA89mE6jdPyHOEzQnaYDLInKFPGxw&_hsmi=329822401&utm_content=329822401&utm_source=hs_automation\n",
    "OPENAI_API_KEY = \"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\"  # e.g., 'sk-...'\n",
    "UNPAYWALL_EMAIL = \"yunruilu@caltech.edu\"  # Email for Unpaywall API\n",
    "NCBI_EMAIL = \"yunruilu@caltech.edu\"  # Email for NCBI Entrez (required by NCBI)\n",
    "NCBI_API_KEY = '903b8602ced7c96ae73650c3ff78350a100'  # Optional: NCBI API Key for higher rate limits, or None | https://support.nlm.nih.gov/kbArticle/?pn=KA-05317\n",
    "\n",
    "# Model and other options\n",
    "OPENAI_MODEL = \"gpt-5\"  # Use 'gpt-4' for best results; 'gpt-3.5-turbo' if lower cost is desired\n",
    "INSTITUTIONAL_ACCESS = True  # True if running on a network with institutional access to paywalled PDFs\n",
    "\n",
    "# Search query and settings\n",
    "SEARCH_QUERY = '(\"spatial transcriptomics\" OR Visium OR MERFISH OR seqFISH OR CosMX OR Xenium)'\n",
    "SEARCH_QUERY = 'spatial transcriptomics'\n",
    "# FIELDS_OF_STUDY = \"Biology\"  # Restrict search to biology-related papers\n",
    "FIELDS_OF_STUDY = None\n",
    "# SEARCH_LIMIT = 100  # max results per API call (Semantic Scholar allows up to 100)\n",
    "# SINCE_DAYS = 7  # default to search for papers in the last 7 days (for weekly run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0d7151e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created empty DataFrame with columns: ['title', 'year', 'venue', 'paper_id', 'doi', 'publication_date', 'oa_pdf_url', 'abstract', 'tldr', 'reference']\n",
      "Saved to: /resnick/groups/mthomson/yunruilu/Github_repo/spatial-genomics-llm-collection/Papers.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create an empty DataFrame with the specified columns\n",
    "columns = [\n",
    "    'title',\n",
    "    'year', \n",
    "    'venue',\n",
    "    'paper_id',\n",
    "    'doi',\n",
    "    'publication_date',\n",
    "    'oa_pdf_url',\n",
    "    'abstract',\n",
    "    'tldr',\n",
    "    'reference'\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Save to the specified path\n",
    "output_path = '/resnick/groups/mthomson/yunruilu/Github_repo/spatial-genomics-llm-collection/Papers.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Created empty DataFrame with columns: {list(df.columns)}\")\n",
    "print(f\"Saved to: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_papers_bulk.py\n",
    "import requests, datetime, time, config\n",
    "from tqdm import tqdm\n",
    "\n",
    "def _get_with_backoff(url, params, headers, max_retries=5, timeout=30):\n",
    "    delay = 1.0\n",
    "    for _ in range(max_retries):\n",
    "        r = requests.get(url, params=params, headers=headers, timeout=timeout)\n",
    "        if r.status_code in (429,) or 500 <= r.status_code < 600:\n",
    "            time.sleep(delay); delay = min(delay * 2, 30); continue\n",
    "        return r\n",
    "    return r\n",
    "\n",
    "def search_new_papers_bulk(since_days=365):\n",
    "    since_date = datetime.date.today() - datetime.timedelta(days=since_days)\n",
    "    url = \"https://api.semanticscholar.org/graph/v1/paper/search/bulk\"\n",
    "\n",
    "    headers = {}\n",
    "    if config.SEMANTIC_SCHOLAR_API_KEY:\n",
    "        headers[\"x-api-key\"] = config.SEMANTIC_SCHOLAR_API_KEY\n",
    "\n",
    "    params = {\n",
    "        # \"query\": config.SEARCH_QUERY,\n",
    "        \"query\": 'spatial transcriptomics',                       # simpler query string\n",
    "        \"fields\": \"title,year,venue,paperId,externalIds,openAccessPdf,publicationDate\",\n",
    "        \"publicationDateOrYear\": f\"{since_date.isoformat()}:{datetime.date.today().isoformat()}\",\n",
    "        \"sort\": \"publicationDate:desc\",\n",
    "        \"limit\": 1000,\n",
    "        \"fieldsOfStudy\": config.FIELDS_OF_STUDY,\n",
    "    }\n",
    "\n",
    "    results, token = [], None\n",
    "    while True:\n",
    "        p = params.copy()\n",
    "        if token: p[\"token\"] = token\n",
    "        resp = _get_with_backoff(url, p, headers)\n",
    "        if resp.status_code != 200:\n",
    "            print(\"API\", resp.status_code, resp.text[:500]); break\n",
    "\n",
    "        data = resp.json()\n",
    "        for paper in data.get(\"data\", []):\n",
    "            pub = paper.get(\"publicationDate\")\n",
    "            doi = (paper.get(\"externalIds\") or {}).get(\"DOI\")\n",
    "            oa  = (paper.get(\"openAccessPdf\") or {}).get(\"url\")\n",
    "            abstract = paper.get(\"abstract\")\n",
    "            tldr = paper.get(\"tldr\")\n",
    "            if doi:\n",
    "                results.append({\n",
    "                    \"title\": paper.get(\"title\",\"\"),\n",
    "                    \"year\": paper.get(\"year\"),\n",
    "                    \"venue\": paper.get(\"venue\",\"\"),\n",
    "                    \"paper_id\": paper.get(\"paperId\"),\n",
    "                    \"doi\": doi,\n",
    "                    \"publication_date\": pub,\n",
    "                    \"oa_pdf_url\": oa,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"tldr\": tldr,\n",
    "                })\n",
    "        token = data.get(\"token\")\n",
    "        if not token: break\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(result) 37\n",
      "{'title': 'Spatial transcriptomics of intraductal carcinoma of the prostate.', 'year': 2025, 'venue': 'Histopathology', 'paper_id': '9673ebf76c2e0eb9d6b742f4f649f0982e5e6c82', 'doi': '10.1111/his.15551', 'publication_date': '2025-09-18', 'oa_pdf_url': '', 'abstract': None, 'tldr': None}\n"
     ]
    }
   ],
   "source": [
    "result = search_new_papers_bulk(since_days = 10)\n",
    "print('len(result)', len(result))\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4975552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_references(paper_id, fields=\"citedPaper.paperId,citedPaper.externalIds\", max_per_page=1000):\n",
    "    \"\"\"\n",
    "    Robustly fetch references for a paper, returning a list of citedPaper dicts.\n",
    "    Handles 'data': null, pagination via 'next', and non-200 responses.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.semanticscholar.org/graph/v1/paper/{paper_id}/references\"\n",
    "    headers = {\"x-api-key\": config.SEMANTIC_SCHOLAR_API_KEY} if config.SEMANTIC_SCHOLAR_API_KEY else {}\n",
    "    params = {\"fields\": fields, \"limit\": max_per_page, \"offset\": 0}\n",
    "    out = []\n",
    "    while True:\n",
    "        r = _get_with_backoff(url, params, headers)\n",
    "        if r.status_code != 200:\n",
    "            # Surface API response text to help debugging auth/rate-limit/etc.\n",
    "            raise RuntimeError(f\"S2 references error {r.status_code}: {r.text[:300]}\")\n",
    "        try:\n",
    "            data = r.json()\n",
    "        except ValueError:\n",
    "            raise RuntimeError(\"S2 references returned non-JSON response\")\n",
    "\n",
    "        items = data.get(\"data\") or []  # <- key fix: 'null' -> []\n",
    "        # Each item normally has {'citedPaper': {...}}; fall back defensively.\n",
    "        for row in items:\n",
    "            cp = (row or {}).get(\"citedPaper\") or row or {}\n",
    "            out.append(cp)\n",
    "\n",
    "        nxt = data.get(\"next\")\n",
    "        if nxt is None:\n",
    "            break\n",
    "        params[\"offset\"] = nxt\n",
    "    return out\n",
    "\n",
    "def get_cited_papers(paper_id: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Return a list of {\"paper_id\": <str>, \"doi\": <str|None>} for all papers\n",
    "    that the given paper_id cites.\n",
    "    \"\"\"\n",
    "    refs = get_references(\n",
    "        paper_id,\n",
    "        fields=\"citedPaper.paperId,citedPaper.externalIds\",\n",
    "        max_per_page=1000\n",
    "    )\n",
    "\n",
    "    out, seen = [], set()\n",
    "    for cp in refs:  # cp is the cited paper dict\n",
    "        pid = cp.get(\"paperId\")\n",
    "        doi = (cp.get(\"externalIds\") or {}).get(\"DOI\")\n",
    "        key = (pid, (doi or \"\").lower())\n",
    "        if pid and key not in seen:\n",
    "            out.append({\"paper_id\": pid, \"doi\": (doi.lower() if doi else None)})\n",
    "            seen.add(key)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:36,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, one_paper in tqdm(enumerate(result)):\n",
    "    items = get_cited_papers(one_paper['paper_id'])\n",
    "    result[i]['reference'] = items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c65482cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Systematic benchmarking of computational methods to identify spatially variable genes', 'year': 2025, 'venue': 'Genome Biology', 'paper_id': '22f81f296713100ec8164688d7bb7d98a5576510', 'doi': '10.1186/s13059-025-03731-2', 'publication_date': '2025-09-18', 'oa_pdf_url': '', 'abstract': None, 'tldr': None, 'reference': [{'paper_id': 'fcbea9ca7bcbe8283456e0913f15b35f4bfed8b9', 'doi': '10.21203/rs.3.rs-4181617/v1'}, {'paper_id': 'f0278e4159224afafa27697267b2f62245dc4cae', 'doi': '10.1038/s41588-024-01664-3'}, {'paper_id': '4845e9f3292db0e8fd7ea3d60a5914ef2280e50a', 'doi': '10.1186/s13059-023-03145-y'}, {'paper_id': 'ab0f9ebab25d53a9f4633de0045235b8f08ec548', 'doi': '10.1186/s13059-023-03045-1'}, {'paper_id': 'dfb56d7fc358f7d1900e1e90dd2b1da56c509c81', 'doi': '10.1101/2022.05.16.492124'}, {'paper_id': '7740e268273b187df4927d58bd2396516d89388e', 'doi': '10.1038/s41587-023-01772-1'}, {'paper_id': '6d0c930590c0c0327fe698a6e5667ee7800ed6f1', 'doi': '10.1101/2023.04.01.535228'}, {'paper_id': '4a73897d6556e39ef156992b8959aa4b9e592deb', 'doi': '10.1038/s41576-023-00586-w'}, {'paper_id': 'b35dace6a2a571adeedc44132a56b0d40b3c2804', 'doi': '10.1101/2022.12.10.519929'}, {'paper_id': 'c393cbee49d6791f7dbc4bdbba714407611546cf', 'doi': '10.1101/2023.03.10.532127'}, {'paper_id': '36a0049e275ef99a4f2b7ea0b09ab63def53125b', 'doi': '10.1038/s41467-023-36796-3'}, {'paper_id': '28f95aca6045692315ff80c2e9f0964ac8b7fdc1', 'doi': '10.1038/s41587-022-01603-9'}, {'paper_id': 'e9ff812d4ac6b6b7eaf82fa2c61aea8da0b652db', 'doi': '10.1038/s41467-023-40271-4'}, {'paper_id': 'acb9c3cf785a6b61f6eacab1bc69218862acc009', 'doi': '10.1038/s41586-022-05094-1'}, {'paper_id': 'e6bbf162df74bef3a74d3bf5d8585db53518c5b8', 'doi': '10.1038/s41592-021-01358-2'}, {'paper_id': '893fbe40b3cbde4fc48581e8cc8855e945cc655c', 'doi': '10.1038/s41467-022-34879-1'}, {'paper_id': '1d3223f68d6196c9fa9ec01f4a9d2299c7de1027', 'doi': '10.1038/s41587-021-01139-4'}, {'paper_id': 'd90e5875fb46b69aff259c1b6b8c6bd83f7dd06e', 'doi': '10.1038/s41592-021-01282-5'}, {'paper_id': '13d799bf2bcd52ff7021ca799be9aab84c44f714', 'doi': '10.1038/s41467-021-26614-z'}, {'paper_id': '5152c6254aed0bb3c5fca2c9cfaf1fb3d52971bb', 'doi': '10.1038/s41592-021-01264-7'}, {'paper_id': '85b6c6d0bfcd44a5114963255b9a9ba641054db7', 'doi': '10.1101/2021.10.27.466045'}, {'paper_id': '31decbf977c355ff908c8e5107fbba6fe904b932', 'doi': '10.1038/s41467-021-26271-2'}, {'paper_id': 'ce7802b798dabf8abad687f1b91391c146b16e0b', 'doi': '10.1038/s41587-021-01044-w'}, {'paper_id': '92a916a74ac7f6139a700e78d3e22fee3b2cf6a4', 'doi': '10.1186/s13059-021-02404-0'}, {'paper_id': '912c676847379784ab311a1f01df9497cdd8de11', 'doi': '10.1038/s41587-021-00935-2'}, {'paper_id': 'af7225a8c839669b0c3e1e4cd07f63ac09557627', 'doi': '10.1186/s12864-022-08601-w'}, {'paper_id': 'dc44a01391c4967eac04a565a36c07c09c6af867', 'doi': '10.1093/bioinformatics/btab164'}, {'paper_id': 'a5dcc6b0d51668115a3382468382e074d053801a', 'doi': '10.1016/j.cell.2022.04.003'}, {'paper_id': 'bff639f7b02be17dca93ee0d8515144fbd79944e', 'doi': '10.12688/f1000research.29032.2'}, {'paper_id': '4ffbc31165cb0aa67b9fec24d7012d31c9a9fc61', 'doi': '10.1101/2020.12.10.419549'}, {'paper_id': 'ce941ee347916bf15d0bc03db1af539c83ced0b3', 'doi': '10.1038/s41586-022-05060-x'}, {'paper_id': '89d53f6d5896b40287bbd4d9c2675debc0fe0b4d', 'doi': '10.1038/s41587-020-0739-1'}, {'paper_id': '4ae9a8158bb92bab66b3b0252f6d760f2f59dae4', 'doi': '10.1093/bioinformatics/btab455'}, {'paper_id': 'eba4d084b57516f22c5b31fa426859634c1f33d2', 'doi': '10.1093/bioinformatics/btab486'}, {'paper_id': '2f06354848bbdb52231bddc252c7a79ddba36125', 'doi': '10.1038/s41587-021-00830-w'}, {'paper_id': '86ad0695623e8964d06d97dd88b8958b566751de', 'doi': '10.1038/s41593-020-00787-0'}, {'paper_id': 'a6c405c30b562bf779108d3b229d7556fad508c5', 'doi': '10.1038/s41592-019-0701-7'}, {'paper_id': '46c7c73f86aef037a5a0ab770000c11da9e7b6e5', 'doi': '10.1038/s41592-019-0548-y'}, {'paper_id': '7045f05312b80bd7e995b5dad637938d626c3be2', 'doi': '10.1126/science.aaw1219'}, {'paper_id': 'be27d34776a627f1dcb05de245ffc53d0d55bda7', 'doi': '10.1038/s41467-022-33182-3'}, {'paper_id': 'c592384802c2a69304f9e21cb03039be80b7bfac', 'doi': '10.1101/482240'}, {'paper_id': 'a63b777e1910c3909883877ec443b7a1f123aa9e', 'doi': '10.1080/01621459.2018.1554485'}, {'paper_id': '8d280e0eb316928ff8c0fd46e817fd395ce6ae71', 'doi': '10.1126/science.aat5691'}, {'paper_id': 'f5f912c3f668c6202d32acc3d85c86cb18a25c21', 'doi': '10.1038/nmeth.4636'}, {'paper_id': '36eff562f65125511b5dfab68ce7f7a943c27478', 'doi': None}, {'paper_id': '2724bfeabb90eff9f7bb1efd2cb9497afe4cbf08', 'doi': '10.1126/science.aaf2403'}, {'paper_id': 'd7b42a977e9c09654b5a4e65538fb56bc9252927', 'doi': '10.1126/science.aaa6090'}, {'paper_id': '62b839dcb0eee3e8487a436bfb367a283283ea7d', 'doi': '10.1093/bioinformatics/btv153'}, {'paper_id': '9bc923b60f93ce314e6db6449ac877693168c952', 'doi': '10.1038/nmeth.2892'}, {'paper_id': '42abddd227d653a0375d7d037ddb885f6c07f66f', 'doi': '10.1186/gb-2008-9-9-r137'}, {'paper_id': '6c8042db93da15d5d8bc49a5be326672cadbf4df', 'doi': '10.1093/biomet/37.1-2.17'}]}\n"
     ]
    }
   ],
   "source": [
    "print(result[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c807b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch_pdf_pipeline.py\n",
    "import os, re, io, time, urllib.parse, requests, fitz\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, Tuple\n",
    "import sys\n",
    "sys.path.append('/resnick/groups/mthomson/yunruilu/Github_repo/spatial-genomics-llm-collection')\n",
    "import config\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "\n",
    "def _safe_filename_from_doi(doi: str) -> str:\n",
    "    # Make a safe filename from the DOI\n",
    "    # e.g., 10.1038/s41586-019-1049-y -> 10.1038_s41586-019-1049-y.pdf\n",
    "    return re.sub(r'[^A-Za-z0-9._-]+', '_', doi) + \".pdf\"\n",
    "\n",
    "def _extract_text_from_pdf_bytes(pdf_bytes: bytes) -> Optional[str]:\n",
    "    try:\n",
    "        doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "    except Exception:\n",
    "        return None\n",
    "    try:\n",
    "        parts = []\n",
    "        for page in doc:\n",
    "            parts.append(page.get_text())  # plain text extraction\n",
    "        return \"\".join(parts)\n",
    "    finally:\n",
    "        doc.close()\n",
    "\n",
    "def _download_ok(resp: requests.Response) -> bool:\n",
    "    ctype = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
    "    return resp.status_code == 200 and (resp.content and (\"pdf\" in ctype or resp.content[:4] == b\"%PDF\"))\n",
    "\n",
    "# ---------- Main function ----------\n",
    "\n",
    "def fetch_pdf_and_text_by_doi(\n",
    "    doi: str,\n",
    "    save_dir: str = \"/resnick/groups/mthomson/yunruilu/Github_repo/spatial-genomics-llm-collection/PDF\",\n",
    "    oa_pdf_url: Optional[str] = None,\n",
    "    openathens_prefix: str = \"https://go.openathens.net/redirector/caltech.edu?url=\",\n",
    "    session: Optional[requests.Session] = None,\n",
    "    timeout: int = 30,\n",
    ") -> Tuple[Optional[str], Optional[str]]:\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    pdf_bytes = None\n",
    "\n",
    "    # 1) Direct OA URL\n",
    "    if oa_pdf_url:\n",
    "        print(f\"Trying method 1: Direct OA URL for DOI {doi}\")\n",
    "        try:\n",
    "            r = requests.get(oa_pdf_url, timeout=timeout)\n",
    "            if _download_ok(r):\n",
    "                pdf_bytes = r.content\n",
    "                print(f\"Success at method 1: Direct OA URL for DOI {doi}\")\n",
    "            else:\n",
    "                print(f\"Failed method 1: Direct OA URL for DOI {doi}\")\n",
    "        except Exception:\n",
    "            print(f\"Failed method 1: Direct OA URL for DOI {doi}\")\n",
    "\n",
    "    # 2) Unpaywall\n",
    "    if pdf_bytes is None and config.UNPAYWALL_EMAIL and \"your_email\" not in config.UNPAYWALL_EMAIL:\n",
    "        print(f\"Trying method 2: Unpaywall for DOI {doi}\")\n",
    "        try:\n",
    "            upw_url = f\"https://api.unpaywall.org/v2/{doi}?email={config.UNPAYWALL_EMAIL}\"\n",
    "            ur = requests.get(upw_url, timeout=timeout)\n",
    "            if ur.status_code == 200:\n",
    "                j = ur.json()\n",
    "                pdf_url = (j.get(\"best_oa_location\") or {}).get(\"url_for_pdf\") or (j.get(\"best_oa_location\") or {}).get(\"url\")\n",
    "                if not pdf_url:\n",
    "                    for loc in j.get(\"oa_locations\") or []:\n",
    "                        pdf_url = loc.get(\"url_for_pdf\") or loc.get(\"url\")\n",
    "                        if pdf_url: break\n",
    "                if pdf_url:\n",
    "                    pr = requests.get(pdf_url, timeout=timeout)\n",
    "                    if _download_ok(pr):\n",
    "                        pdf_bytes = pr.content\n",
    "                        print(f\"Success at method 2: Unpaywall for DOI {doi}\")\n",
    "                    else:\n",
    "                        print(f\"Failed method 2: Unpaywall for DOI {doi}\")\n",
    "                else:\n",
    "                    print(f\"Failed method 2: Unpaywall for DOI {doi} (no PDF URL found)\")\n",
    "            else:\n",
    "                print(f\"Failed method 2: Unpaywall for DOI {doi} (API error)\")\n",
    "        except Exception:\n",
    "            print(f\"Failed method 2: Unpaywall for DOI {doi}\")\n",
    "\n",
    "    # 3) DOI content negotiation\n",
    "    if pdf_bytes is None:\n",
    "        print(f\"Trying method 3: DOI content negotiation for DOI {doi}\")\n",
    "        try:\n",
    "            r = requests.get(f\"https://doi.org/{doi}\", headers={\"Accept\": \"application/pdf\"}, timeout=timeout, allow_redirects=True)\n",
    "            if _download_ok(r):\n",
    "                pdf_bytes = r.content\n",
    "                print(f\"Success at method 3: DOI content negotiation for DOI {doi}\")\n",
    "            else:\n",
    "                print(f\"Failed method 3: DOI content negotiation for DOI {doi}\")\n",
    "        except Exception:\n",
    "            print(f\"Failed method 3: DOI content negotiation for DOI {doi}\")\n",
    "\n",
    "    # 4) OpenAthens redirector\n",
    "    if pdf_bytes is None and openathens_prefix:\n",
    "        print(f\"Trying method 4: OpenAthens redirector for DOI {doi}\")\n",
    "        try:\n",
    "            sess = session or requests.Session()\n",
    "            proxied_url = f\"{openathens_prefix}{urllib.parse.quote('https://doi.org/' + doi, safe='')}\"\n",
    "            resp = sess.get(proxied_url, allow_redirects=True, timeout=timeout)\n",
    "            ctype = resp.headers.get(\"Content-Type\", \"\")\n",
    "\n",
    "            if ctype.startswith(\"application/pdf\"):\n",
    "                # Got the PDF directly\n",
    "                pdf_bytes = resp.content\n",
    "                print(f\"Success at method 4: OpenAthens redirector for DOI {doi}\")\n",
    "            elif \"html\" in ctype:\n",
    "                html = resp.text\n",
    "                # Check if this is a login page or the article page by looking for clues\n",
    "                if \"openathens.net\" in html.lower() or \"login\" in resp.url:\n",
    "                    print(f\"Failed method 4: OpenAthens redirector for DOI {doi} (authentication required)\")\n",
    "                else:\n",
    "                    # Assume this is the article page HTML, try to find a PDF link\n",
    "                    match = re.search(r'href=\"([^\"]+\\.pdf[^\"]*)\"', html)\n",
    "                    if match:\n",
    "                        pdf_link = match.group(1)\n",
    "                        # Complete relative link if needed\n",
    "                        if pdf_link.startswith(\"/\"):\n",
    "                            from urllib.parse import urljoin\n",
    "                            pdf_link = urljoin(resp.url, pdf_link)\n",
    "                        pdf_resp = sess.get(pdf_link, timeout=timeout)\n",
    "                        if pdf_resp.headers.get(\"Content-Type\",\"\").startswith(\"application/pdf\"):\n",
    "                            pdf_bytes = pdf_resp.content\n",
    "                            print(f\"Success at method 4: OpenAthens redirector for DOI {doi}\")\n",
    "                        else:\n",
    "                            print(f\"Failed method 4: OpenAthens redirector for DOI {doi} (PDF link didn't return PDF)\")\n",
    "                    else:\n",
    "                        print(f\"Failed method 4: OpenAthens redirector for DOI {doi} (no PDF link found on page)\")\n",
    "            else:\n",
    "                print(f\"Failed method 4: OpenAthens redirector for DOI {doi} (unexpected content type)\")\n",
    "        except Exception:\n",
    "            print(f\"Failed method 4: OpenAthens redirector for DOI {doi}\")\n",
    "\n",
    "    if pdf_bytes is None:\n",
    "        print(f\"All methods failed for DOI {doi}\")\n",
    "        return (None, None)\n",
    "\n",
    "    # Save + extract\n",
    "    pdf_name = _safe_filename_from_doi(doi)\n",
    "    dest = os.path.join(save_dir, pdf_name)\n",
    "    with open(dest, \"wb\") as f:\n",
    "        f.write(pdf_bytes)\n",
    "    text = _extract_text_from_pdf_bytes(pdf_bytes) or \"\"\n",
    "    print(f\"Successfully saved PDF and extracted text for DOI {doi}\")\n",
    "    return (dest, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c202f606",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresult\u001b[49m[\u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "print(result[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc2163f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying method 2: Unpaywall for DOI 10.1038/s41592-025-02773-5\n",
      "Failed method 2: Unpaywall for DOI 10.1038/s41592-025-02773-5 (no PDF URL found)\n",
      "Trying method 3: DOI content negotiation for DOI 10.1038/s41592-025-02773-5\n",
      "Failed method 3: DOI content negotiation for DOI 10.1038/s41592-025-02773-5\n",
      "Trying method 4: OpenAthens redirector for DOI 10.1038/s41592-025-02773-5\n",
      "Success at method 4: OpenAthens redirector for DOI 10.1038/s41592-025-02773-5\n",
      "Successfully saved PDF and extracted text for DOI 10.1038/s41592-025-02773-5\n",
      "/resnick/groups/mthomson/yunruilu/Github_repo/spatial-genomics-llm-collection/PDF/10.1038_s41592-025-02773-5.pdf\n",
      "86249\n",
      "Nature Methods | Volume 22 | September 2025 | 1846–1856\n",
      "1846\n",
      "nature methods\n",
      "Article\n",
      "https://doi.org/10.1038/s41592-025-02773-5\n",
      "Cancer subclone detection based on DNA \n",
      "copy number in single-cell and spatial omic \n",
      "sequencing data\n",
      " \n",
      "Chi-Yun Wu \n",
      "  1,2,7, Jiazhen Rong \n",
      "  1,2,7, Anuja Sathe \n",
      "  3, Paul R. Hess2,4, \n",
      "Billy T. Lau \n",
      "  3,5, Susan M. Grimes \n",
      "  3, Sijia Huang2, Hanlee P. Ji3,6 & \n",
      "Nancy R. Zhang \n",
      "  1,2 \n",
      "Somatic mutations such as copy number alterations accumulate during \n",
      "cancer progression, driving intratumor heterogeneity that impacts \n",
      "therapy effectiveness. Understanding the characteristics and spatial \n",
      "distribution of genetically distinct subclones is essential for unraveling \n",
      "tumor evolution and improving cancer treatment. Here we present \n",
      "Clonalscope, a subclone detection method using copy number profiles, \n",
      "applicable to spatial transcriptomics and single-cell sequencing data. \n",
      "Clonalscope implements a nested Chinese Restaurant Process to identify \n",
      "de novo tumor subclones, which\n"
     ]
    }
   ],
   "source": [
    "doi = '10.1038/s41592-025-02773-5'\n",
    "path, full_text = fetch_pdf_and_text_by_doi(doi)\n",
    "print(path)\n",
    "print(len(full_text))\n",
    "print(full_text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f35a8d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import fitz  # PyMuPDF for PDF parsing\n",
    "from openai import OpenAI\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/resnick/groups/mthomson/yunruilu/Github_repo/spatial-genomics-llm-collection')\n",
    "import config\n",
    "\n",
    "# Initialize OpenAI client with API key\n",
    "client = OpenAI(api_key=config.OPENAI_API_KEY)\n",
    "\n",
    "def extract_datasets_from_text(full_text: str = None, pdf_path: str = None, paper_title: str = None):\n",
    "    \"\"\"\n",
    "    Extract detailed dataset information from a research paper given its text or PDF file path.\n",
    "    \n",
    "    Either `full_text` or `pdf_path` must be provided. If both are provided, `pdf_path` is prioritized.\n",
    "    \n",
    "    Returns:\n",
    "        A list of dictionaries, each containing details about a dataset used in the paper:\n",
    "        [\n",
    "            {\n",
    "                \"data link\": str,        # Direct URL or DOI link to the dataset if available\n",
    "                \"repository\": str,       # Repository name (e.g., GEO, SRA, Zenodo) or 'Not available'\n",
    "                \"accession\": str,        # Accession ID or DOI of the dataset (if applicable)\n",
    "                \"platform\": str,         # Technology platform (e.g., Visium, Xenium, MERFISH, scRNA-seq, CODEX, etc.)\n",
    "                \"species\": str,          # Organism species (if mentioned)\n",
    "                \"tissue\": str,           # Tissue or sample type (if mentioned)\n",
    "                \"raw_data_available\": bool, # True if raw data files are available, False otherwise\n",
    "                \"available\": bool,       # True if the dataset is publicly available, False if restricted/not available\n",
    "                \"description\": str       # Description of the dataset, including platform resolution and origin (generated by this study or from another source)\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if pdf_path is None and full_text is None:\n",
    "        print(\"Error: No input text or PDF path provided.\")\n",
    "        return []\n",
    "    \n",
    "    # Extract text from PDF if a path is provided\n",
    "    text_content = \"\"\n",
    "    if pdf_path is not None:\n",
    "        try:\n",
    "            # Open the PDF and extract all text\n",
    "            doc = fitz.open(pdf_path)\n",
    "            for page in doc:\n",
    "                text_content += page.get_text()\n",
    "            doc.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read PDF file {pdf_path}: {e}\")\n",
    "            return []\n",
    "    else:\n",
    "        text_content = full_text\n",
    "    \n",
    "    if not text_content:\n",
    "        # If text extraction failed or resulted in empty content\n",
    "        print(\"Error: No text content could be extracted from the input.\")\n",
    "        return []\n",
    "    \n",
    "    # Normalize whitespace and remove hyphenation line breaks for better parsing\n",
    "    text_clean = text_content.replace(\"-\\n\", \"\").replace(\"\\n\", \" \")\n",
    "    # Exclude references section to avoid confusion with data DOIs or accessions in references\n",
    "    text_upper = text_clean.upper()\n",
    "    if \"REFERENCES\" in text_upper:\n",
    "        text_body = text_clean[: text_upper.index(\"REFERENCES\")]\n",
    "    else:\n",
    "        text_body = text_clean\n",
    "    \n",
    "    # Prepare the system and user messages for the GPT model\n",
    "    system_msg = (\n",
    "        \"You are an expert assistant extracting dataset information from scientific papers. \"\n",
    "        \"Identify all datasets mentioned in the paper and extract relevant details. \"\n",
    "        \"Include the repository (or source) and accession/ID or DOI for each dataset, the data platform/technology used \"\n",
    "        \"(e.g., 10x Genomics Visium, 10x Xenium, NanoString CosMX, MERFISH, seqFISH, CODEX, or single-cell RNA-seq if applicable), \"\n",
    "        \"the species and tissue, whether raw data is available, whether the dataset is publicly available, and a brief description. \"\n",
    "        \"whether the authors generated the data or reused the data from another source, and a brief description. \"\n",
    "        \"In the description, mention the platform and its resolution (for example, if it's spatial transcriptomics with spot-based or single-cell resolution, or if it's non-spatial single-cell RNA-seq), \"\n",
    "        \"and state whether the dataset was generated in this study or obtained from another source (citing the source or reference if mentioned).\"\n",
    "    )\n",
    "    user_msg = (\n",
    "        \"Extract all datasets (particularly spatially-resolved omics datasets) mentioned in the following text. \"\n",
    "        \"If the paper includes a single-cell RNA-seq dataset (which is non-spatial) for analysis, include it as well and denote it appropriately. \"\n",
    "        \"Return ONLY a valid JSON array of objects, where each object has the keys: \"\n",
    "        \"data link, repository, accession, platform, species, tissue, raw_data_available, available, original_data, description. \"\n",
    "        \"If a dataset is not publicly available (e.g., available upon request or not provided), set repository to \\\"Not available\\\" and available to false. \"\n",
    "        \"Provide no extra commentary or explanation, only the JSON.\\n\\n\"\n",
    "        f\"Text:\\n\\\"\\\"\\\"\\n{text_body}\\n\\\"\\\"\\\"\"\n",
    "    )\n",
    "    \n",
    "    # Call the OpenAI API to get the dataset details in JSON format\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-5\",\n",
    "            # model=\"gpt-4.1\",\n",
    "            # model=config.OPENAI_MODEL,  # e.g., 'gpt-4' or 'gpt-3.5-turbo'\n",
    "            messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                      {\"role\": \"user\",  \"content\": user_msg}],\n",
    "            # temperature=0\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API error: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # The model's answer (should be JSON or contain JSON)\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Helper to parse the JSON from the model's response\n",
    "    def _extract_json(text: str):\n",
    "        # Attempt direct JSON parse\n",
    "        try:\n",
    "            return json.loads(text), None\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Check for JSON in a markdown code block\n",
    "        match = re.search(r\"```(?:json)?\\s*([\\s\\S]+?)```\", text)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group(1)), None\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "        else:\n",
    "            last_err = None\n",
    "        # Fallback: find first JSON object/array in the text\n",
    "        match = re.search(r\"(\\{.*?\\}|\\[.*?\\])\", text, flags=re.DOTALL)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group(1)), None\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "        return None, last_err\n",
    "    \n",
    "    # Parse the JSON content from the model's output\n",
    "    datasets, err = _extract_json(content)\n",
    "    if err or datasets is None:\n",
    "        print(f\"Failed to parse JSON from model output: {err or 'No JSON found'}\")\n",
    "        return []\n",
    "    \n",
    "    # Ensure the result is a list of dicts\n",
    "    if isinstance(datasets, dict):\n",
    "        datasets = [datasets]\n",
    "    \n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dc607b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"data link\": \"Not available\",\n",
      "  \"repository\": \"Not available\",\n",
      "  \"accession\": \"Not available\",\n",
      "  \"platform\": \"Akoya Biosciences PhenoCycler (CODEX) multiplexed immunofluorescence (~40-plex), subcellular resolution\",\n",
      "  \"species\": \"Human\",\n",
      "  \"tissue\": \"Head and neck cancer tumour tissue (FFPE tumour microarray cores)\",\n",
      "  \"raw_data_available\": false,\n",
      "  \"available\": false,\n",
      "  \"original_data\": true,\n",
      "  \"description\": \"UPMC-HNC: 40-plex spatial proteomics (CODEX) imaging of FFPE head-and-neck cancer resections as TMA cores; subcellular resolution. 308 samples from 81 patients with clinical annotations. Dataset generated in this study; data available only upon reasonable request.\"\n",
      "}\n",
      "{\n",
      "  \"data link\": \"Not available\",\n",
      "  \"repository\": \"Not available\",\n",
      "  \"accession\": \"Not available\",\n",
      "  \"platform\": \"Akoya Biosciences PhenoCycler (CODEX) multiplexed immunofluorescence (~40-plex), subcellular resolution\",\n",
      "  \"species\": \"Human\",\n",
      "  \"tissue\": \"Colorectal cancer tumour tissue (FFPE tumour microarray cores)\",\n",
      "  \"raw_data_available\": false,\n",
      "  \"available\": false,\n",
      "  \"original_data\": true,\n",
      "  \"description\": \"Stanford-CRC: 40-plex spatial proteomics (CODEX) imaging of FFPE colorectal cancer resections as TMA cores; subcellular resolution. 292 samples from 161 patients with clinical annotations. Dataset generated in this study; data available only upon reasonable request.\"\n",
      "}\n",
      "{\n",
      "  \"data link\": \"Not available\",\n",
      "  \"repository\": \"Not available\",\n",
      "  \"accession\": \"Not available\",\n",
      "  \"platform\": \"Akoya Biosciences PhenoCycler (CODEX) multiplexed immunofluorescence (~40-plex), subcellular resolution\",\n",
      "  \"species\": \"Human\",\n",
      "  \"tissue\": \"Head and neck cancer tumour tissue (pre- and post-neoadjuvant anti-PD-1 therapy), FFPE tumour microarray cores\",\n",
      "  \"raw_data_available\": false,\n",
      "  \"available\": false,\n",
      "  \"original_data\": true,\n",
      "  \"description\": \"DFCI-HNC: 40-plex spatial proteomics (CODEX) imaging of FFPE head-and-neck cancer biopsies before and after neoadjuvant anti-PD-1 therapy; subcellular resolution. 58 samples from 29 patients with pathologic response annotations. Dataset generated in this study; data available only upon reasonable request.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "datasets_info = extract_datasets_from_text(pdf_path=\"/resnick/groups/mthomson/yunruilu/Github_repo/spatial-genomics-llm-collection/PDF/10.1038_s41551-022-00951-w.pdf\")\n",
    "\n",
    "for ds in datasets_info:\n",
    "    print(json.dumps(ds, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e095da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'data link': 'Not available',\n",
       "  'repository': 'Not available',\n",
       "  'accession': 'Not available',\n",
       "  'platform': 'Akoya Biosciences PhenoCycler (CODEX) multiplexed immunofluorescence (~40-plex), subcellular resolution',\n",
       "  'species': 'Human',\n",
       "  'tissue': 'Head and neck cancer tumour tissue (FFPE tumour microarray cores)',\n",
       "  'raw_data_available': False,\n",
       "  'available': False,\n",
       "  'original_data': True,\n",
       "  'description': 'UPMC-HNC: 40-plex spatial proteomics (CODEX) imaging of FFPE head-and-neck cancer resections as TMA cores; subcellular resolution. 308 samples from 81 patients with clinical annotations. Dataset generated in this study; data available only upon reasonable request.'},\n",
       " {'data link': 'Not available',\n",
       "  'repository': 'Not available',\n",
       "  'accession': 'Not available',\n",
       "  'platform': 'Akoya Biosciences PhenoCycler (CODEX) multiplexed immunofluorescence (~40-plex), subcellular resolution',\n",
       "  'species': 'Human',\n",
       "  'tissue': 'Colorectal cancer tumour tissue (FFPE tumour microarray cores)',\n",
       "  'raw_data_available': False,\n",
       "  'available': False,\n",
       "  'original_data': True,\n",
       "  'description': 'Stanford-CRC: 40-plex spatial proteomics (CODEX) imaging of FFPE colorectal cancer resections as TMA cores; subcellular resolution. 292 samples from 161 patients with clinical annotations. Dataset generated in this study; data available only upon reasonable request.'},\n",
       " {'data link': 'Not available',\n",
       "  'repository': 'Not available',\n",
       "  'accession': 'Not available',\n",
       "  'platform': 'Akoya Biosciences PhenoCycler (CODEX) multiplexed immunofluorescence (~40-plex), subcellular resolution',\n",
       "  'species': 'Human',\n",
       "  'tissue': 'Head and neck cancer tumour tissue (pre- and post-neoadjuvant anti-PD-1 therapy), FFPE tumour microarray cores',\n",
       "  'raw_data_available': False,\n",
       "  'available': False,\n",
       "  'original_data': True,\n",
       "  'description': 'DFCI-HNC: 40-plex spatial proteomics (CODEX) imaging of FFPE head-and-neck cancer biopsies before and after neoadjuvant anti-PD-1 therapy; subcellular resolution. 58 samples from 29 patients with pathologic response annotations. Dataset generated in this study; data available only upon reasonable request.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b87eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174d4941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7286142f",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Invalid schema for response_format 'datasets': In context=('properties', 'datasets', 'items'), 'required' is required to be supplied and to be an array including every key in properties. Missing 'data_link'.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1) If you have raw text already:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_from_text \u001b[38;5;241m=\u001b[39m \u001b[43mextract_datasets_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(data_from_text, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[0;32mIn[40], line 603\u001b[0m, in \u001b[0;36mextract_datasets_from_text\u001b[0;34m(full_text)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_datasets_from_text\u001b[39m(full_text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    599\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    600\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: SYSTEM},\n\u001b[1;32m    601\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: USER_PREFIX \u001b[38;5;241m+\u001b[39m full_text}\n\u001b[1;32m    602\u001b[0m     ]\n\u001b[0;32m--> 603\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRESPONSE_FORMAT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m     obj \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(resp\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n",
      "File \u001b[0;32m/resnick/groups/mthomson/yunruilu/miniconda3/envs/Paper_Collection/lib/python3.9/site-packages/openai/_utils/_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/resnick/groups/mthomson/yunruilu/miniconda3/envs/Paper_Collection/lib/python3.9/site-packages/openai/resources/chat/completions/completions.py:1150\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   1106\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m   1148\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m   1149\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_cache_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msafety_identifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverbosity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweb_search_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/resnick/groups/mthomson/yunruilu/miniconda3/envs/Paper_Collection/lib/python3.9/site-packages/openai/_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1258\u001b[0m     )\n\u001b[0;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/resnick/groups/mthomson/yunruilu/miniconda3/envs/Paper_Collection/lib/python3.9/site-packages/openai/_base_client.py:1047\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1046\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1047\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Invalid schema for response_format 'datasets': In context=('properties', 'datasets', 'items'), 'required' is required to be supplied and to be an array including every key in properties. Missing 'data_link'.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}"
     ]
    }
   ],
   "source": [
    "# 1) If you have raw text already:\n",
    "data_from_text = extract_datasets_from_text(full_text=full_text)\n",
    "print(json.dumps(data_from_text, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11468f9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create() got an unexpected keyword argument 'response_format'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 2) If you have a local PDF:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_from_pdf \u001b[38;5;241m=\u001b[39m \u001b[43mextract_datasets_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/resnick/groups/mthomson/yunruilu/Github_repo/spatial-genomics-llm-collection/PDF/10.1186_s13059-025-03731-2.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(data_from_pdf, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[0;32mIn[40], line 673\u001b[0m, in \u001b[0;36mextract_datasets_from_pdf\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m    670\u001b[0m client\u001b[38;5;241m.\u001b[39mvector_stores\u001b[38;5;241m.\u001b[39mfiles\u001b[38;5;241m.\u001b[39mcreate(vector_store_id\u001b[38;5;241m=\u001b[39mvs\u001b[38;5;241m.\u001b[39mid, file_id\u001b[38;5;241m=\u001b[39mf\u001b[38;5;241m.\u001b[39mid)\n\u001b[1;32m    672\u001b[0m \u001b[38;5;66;03m# 2) Call the Responses API with file_search tool + vector_store_ids\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPROMPT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile_search\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvector_store_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_num_results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRESPONSE_FORMAT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# (optional) include retrieved spans for auditing:\u001b[39;49;00m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# include=[\"output[*].file_search_call.search_results\"]\u001b[39;49;00m\n\u001b[1;32m    684\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;66;03m# 3) Parse structured output\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m# In Responses, the text is in output[n].content[0].text\u001b[39;00m\n\u001b[1;32m    688\u001b[0m text_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: create() got an unexpected keyword argument 'response_format'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) If you have a local PDF:\n",
    "data_from_pdf = extract_datasets_from_pdf(pdf_path=\"/resnick/groups/mthomson/yunruilu/Github_repo/spatial-genomics-llm-collection/PDF/10.1186_s13059-025-03731-2.pdf\")\n",
    "print(json.dumps(data_from_pdf, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bd5f629d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'data link': 'https://nda.nih.gov/abcd',\n",
       "  'repository': 'NIMH Data Archive (NDA)',\n",
       "  'accession': 'ABCD',\n",
       "  'platform': 'MRI (T1-weighted, diffusion MRI, resting-state fMRI)',\n",
       "  'species': 'Homo sapiens',\n",
       "  'tissue': 'Brain (whole cortex, children aged 9-10 years)',\n",
       "  'raw_data_available': True,\n",
       "  'available': True,\n",
       "  'description': 'Spatially resolved multimodal neuroimaging dataset from the Adolescent Brain Cognitive Development (ABCD) study, including T1-weighted, diffusion, and resting-state functional MRI for 7,025 children. Used to construct individual-level structural and functional connectomes and gradients for spatial transcriptomics and structure-function coupling analyses.'},\n",
       " {'data link': 'https://db.humanconnectome.org',\n",
       "  'repository': 'Human Connectome Project (HCP)',\n",
       "  'accession': 'HCP Young Adult',\n",
       "  'platform': 'MRI (T1-weighted, T2-weighted, diffusion MRI, resting-state fMRI)',\n",
       "  'species': 'Homo sapiens',\n",
       "  'tissue': 'Brain (whole cortex, adults aged 22-35 years)',\n",
       "  'raw_data_available': True,\n",
       "  'available': True,\n",
       "  'description': 'Spatially resolved multimodal neuroimaging dataset from the Human Connectome Project (HCP) Young Adult cohort, including T1-weighted, T2-weighted, diffusion, and resting-state functional MRI for 913 adults. Used to construct individual-level structural and functional connectomes and gradients for spatial transcriptomics and structure-function coupling analyses.'}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_datasets_from_text(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6d2e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446cc3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803da3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 37 new papers to the CSV file\n",
      "Total papers in CSV after deduplication: 37\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>venue</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>doi</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>oa_pdf_url</th>\n",
       "      <th>abstract</th>\n",
       "      <th>tldr</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spatial transcriptomics of intraductal carcino...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Histopathology</td>\n",
       "      <td>9673ebf76c2e0eb9d6b742f4f649f0982e5e6c82</td>\n",
       "      <td>10.1111/his.15551</td>\n",
       "      <td>2025-09-18</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anatomic Predilection of IDH-Mutant Gliomas: A...</td>\n",
       "      <td>2025</td>\n",
       "      <td>medRxiv</td>\n",
       "      <td>3eb894538844ac916ecd4f91bf9b3d65f8e183cf</td>\n",
       "      <td>10.1101/2025.09.16.25333605</td>\n",
       "      <td>2025-09-18</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brain Functional-Structural Gradient Coupling ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>medRxiv</td>\n",
       "      <td>377d977c7b456afd9b806d642bc46c324ab53681</td>\n",
       "      <td>10.1101/2025.09.16.25335918</td>\n",
       "      <td>2025-09-18</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Systematic benchmarking of computational metho...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Genome Biology</td>\n",
       "      <td>22f81f296713100ec8164688d7bb7d98a5576510</td>\n",
       "      <td>10.1186/s13059-025-03731-2</td>\n",
       "      <td>2025-09-18</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'paper_id': 'fcbea9ca7bcbe8283456e0913f15b35...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sclerotic GVHD and Scleroderma Share Dysregula...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Blood</td>\n",
       "      <td>f0104be4ab11d99e37c8195f0203b68647678fcd</td>\n",
       "      <td>10.1182/blood.2025029836</td>\n",
       "      <td>2025-09-17</td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  year           venue  \\\n",
       "0  Spatial transcriptomics of intraductal carcino...  2025  Histopathology   \n",
       "1  Anatomic Predilection of IDH-Mutant Gliomas: A...  2025         medRxiv   \n",
       "2  Brain Functional-Structural Gradient Coupling ...  2025         medRxiv   \n",
       "3  Systematic benchmarking of computational metho...  2025  Genome Biology   \n",
       "4  Sclerotic GVHD and Scleroderma Share Dysregula...  2025           Blood   \n",
       "\n",
       "                                   paper_id                          doi  \\\n",
       "0  9673ebf76c2e0eb9d6b742f4f649f0982e5e6c82            10.1111/his.15551   \n",
       "1  3eb894538844ac916ecd4f91bf9b3d65f8e183cf  10.1101/2025.09.16.25333605   \n",
       "2  377d977c7b456afd9b806d642bc46c324ab53681  10.1101/2025.09.16.25335918   \n",
       "3  22f81f296713100ec8164688d7bb7d98a5576510   10.1186/s13059-025-03731-2   \n",
       "4  f0104be4ab11d99e37c8195f0203b68647678fcd     10.1182/blood.2025029836   \n",
       "\n",
       "  publication_date oa_pdf_url abstract  tldr  \\\n",
       "0       2025-09-18                None  None   \n",
       "1       2025-09-18                None  None   \n",
       "2       2025-09-18                None  None   \n",
       "3       2025-09-18                None  None   \n",
       "4       2025-09-17                None  None   \n",
       "\n",
       "                                           reference  \n",
       "0                                                 []  \n",
       "1                                                 []  \n",
       "2                                                 []  \n",
       "3  [{'paper_id': 'fcbea9ca7bcbe8283456e0913f15b35...  \n",
       "4                                                 []  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = \"/resnick/groups/mthomson/yunruilu/Github_repo/spatial-genomics-llm-collection/Papers.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "new_papers_df = pd.DataFrame(result)\n",
    "df_updated = pd.concat([df, new_papers_df], ignore_index=True)\n",
    "df_updated = df_updated.drop_duplicates(subset=['paper_id'], keep='first')\n",
    "\n",
    "df_updated.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Added {len(new_papers_df)} new papers to the CSV file\")\n",
    "print(f\"Total papers in CSV after deduplication: {len(df_updated)}\")\n",
    "print(display(df_updated.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ef80a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99403ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d58ed14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "162765d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37 new papers in the last 10 days since today: 2025-09-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Processing paper 1 of 37\n",
      "Trying method 2: Unpaywall for DOI 10.1111/his.15551\n",
      "Failed method 2: Unpaywall for DOI 10.1111/his.15551 (API error)\n",
      "Trying method 3: DOI content negotiation for DOI 10.1111/his.15551\n",
      "Failed method 3: DOI content negotiation for DOI 10.1111/his.15551\n",
      "Trying method 4: OpenAthens redirector for DOI 10.1111/his.15551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed method 4: OpenAthens redirector for DOI 10.1111/his.15551 (no PDF link found on page)\n",
      "All methods failed for DOI 10.1111/his.15551\n",
      "--------------------------------\n",
      "Processing paper 2 of 37\n",
      "Trying method 2: Unpaywall for DOI 10.1101/2025.09.16.25333605\n",
      "Failed method 2: Unpaywall for DOI 10.1101/2025.09.16.25333605 (API error)\n",
      "Trying method 3: DOI content negotiation for DOI 10.1101/2025.09.16.25333605\n",
      "Failed method 3: DOI content negotiation for DOI 10.1101/2025.09.16.25333605\n",
      "Trying method 4: OpenAthens redirector for DOI 10.1101/2025.09.16.25333605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:11, 11.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success at method 4: OpenAthens redirector for DOI 10.1101/2025.09.16.25333605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 587\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    586\u001b[0m \u001b[38;5;66;03m#############################################################################\u001b[39;00m\n\u001b[0;32m--> 587\u001b[0m path, full_text \u001b[38;5;241m=\u001b[39m fetch_pdf_and_text_by_doi(doi \u001b[38;5;241m=\u001b[39m one_paper[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoi\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_text:\n\u001b[1;32m    589\u001b[0m     data_info \u001b[38;5;241m=\u001b[39m extract_datasets_from_text(full_text \u001b[38;5;241m=\u001b[39m full_text)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# import argparse\n",
    "\n",
    "# # Parse command line arguments\n",
    "# parser = argparse.ArgumentParser(description='Search and analyze spatial genomics papers')\n",
    "# parser.add_argument('--since_days', type=int, default=10, help='Number of days to search back')\n",
    "# parser.add_argument('--print_details', type=str, default='True', help='Whether to print details (True/False)')\n",
    "# parser.add_argument('--model', type=str, default='gpt-5', help='OpenAI model to use')\n",
    "# parser.add_argument('--search_query', type=str, default='spatial transcriptomics', help='Search query', nargs='+')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# # Convert string to boolean for print_details\n",
    "# print_details_input = args.print_details.lower() in ['true', '1', 'yes', 'on']\n",
    "# since_days_input = args.since_days\n",
    "# model_input = args.model\n",
    "# search_query_input = ' '.join(args.search_query)\n",
    "\n",
    "print_details_input = True\n",
    "since_days_input = 10\n",
    "model_input = 'gpt-5'\n",
    "search_query_input = 'spatial transcriptomics'\n",
    "\n",
    "SEMANTIC_SCHOLAR_API_KEY = \"19tQFoyv7w5xBQNMsUA7C5lwNqEni5g3GKkP8Pkj\"  # e.g., 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' | https://www.semanticscholar.org/product/api/tutorial?utm_campaign=API%20transaction&utm_medium=email&_hsenc=p2ANqtz--KbD5dVfVRom22kVjKkL-55Ikb73h1Nze5JYW6_8OGfj15Pf_Z7OjRXzHnO2BntuA89mE6jdPyHOEzQnaYDLInKFPGxw&_hsmi=329822401&utm_content=329822401&utm_source=hs_automation\n",
    "OPENAI_API_KEY = \"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\"  # e.g., 'sk-...'\n",
    "UNPAYWALL_EMAIL = \"yunruilu@caltech.edu\"  # Email for Unpaywall API\n",
    "NCBI_EMAIL = \"yunruilu@caltech.edu\"  # Email for NCBI Entrez (required by NCBI)\n",
    "NCBI_API_KEY = '903b8602ced7c96ae73650c3ff78350a100'  # Optional: NCBI API Key for higher rate limits, or None | https://support.nlm.nih.gov/kbArticle/?pn=KA-05317\n",
    "\n",
    "# Model and other options\n",
    "# OPENAI_MODEL = \"gpt-5\"  # Use 'gpt-4' for best results; 'gpt-3.5-turbo' if lower cost is desired\n",
    "INSTITUTIONAL_ACCESS = True  # True if running on a network with institutional access to paywalled PDFs\n",
    "\n",
    "# Search query and settings\n",
    "# SEARCH_QUERY = '(\"spatial transcriptomics\" OR Visium OR MERFISH OR seqFISH OR CosMX OR Xenium)'\n",
    "SEARCH_QUERY = search_query_input\n",
    "# FIELDS_OF_STUDY = \"Biology\"  # Restrict search to biology-related papers\n",
    "FIELDS_OF_STUDY = None\n",
    "\n",
    "# search_papers_bulk.py\n",
    "import requests, datetime, time\n",
    "from tqdm import tqdm\n",
    "# fetch_pdf_pipeline.py\n",
    "import os, re, io, time, urllib.parse, requests, fitz\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, Tuple\n",
    "import re\n",
    "import json\n",
    "import fitz  # PyMuPDF for PDF parsing\n",
    "from openai import OpenAI\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def _get_with_backoff(url, params, headers, max_retries=5, timeout=30):\n",
    "    delay = 1.0\n",
    "    for _ in range(max_retries):\n",
    "        r = requests.get(url, params=params, headers=headers, timeout=timeout)\n",
    "        if r.status_code in (429,) or 500 <= r.status_code < 600:\n",
    "            time.sleep(delay); delay = min(delay * 2, 30); continue\n",
    "        return r\n",
    "    return r\n",
    "\n",
    "def search_new_papers_bulk(since_days=365):\n",
    "    since_date = datetime.date.today() - datetime.timedelta(days=since_days)\n",
    "    url = \"https://api.semanticscholar.org/graph/v1/paper/search/bulk\"\n",
    "\n",
    "    headers = {}\n",
    "    if SEMANTIC_SCHOLAR_API_KEY:\n",
    "        headers[\"x-api-key\"] = SEMANTIC_SCHOLAR_API_KEY\n",
    "\n",
    "    params = {\n",
    "        \"query\": SEARCH_QUERY,\n",
    "        # \"query\": 'spatial transcriptomics',                       # simpler query string\n",
    "        \"fields\": \"title,year,venue,paperId,externalIds,openAccessPdf,publicationDate\",\n",
    "        \"publicationDateOrYear\": f\"{since_date.isoformat()}:{datetime.date.today().isoformat()}\",\n",
    "        \"sort\": \"publicationDate:desc\",\n",
    "        \"limit\": 1000,\n",
    "        \"fieldsOfStudy\": FIELDS_OF_STUDY,\n",
    "    }\n",
    "\n",
    "    results, token = [], None\n",
    "    while True:\n",
    "        p = params.copy()\n",
    "        if token: p[\"token\"] = token\n",
    "        resp = _get_with_backoff(url, p, headers)\n",
    "        if resp.status_code != 200:\n",
    "            print(\"API\", resp.status_code, resp.text[:500]); break\n",
    "\n",
    "        data = resp.json()\n",
    "        for paper in data.get(\"data\", []):\n",
    "            pub = paper.get(\"publicationDate\")\n",
    "            doi = (paper.get(\"externalIds\") or {}).get(\"DOI\")\n",
    "            oa  = (paper.get(\"openAccessPdf\") or {}).get(\"url\")\n",
    "            abstract = paper.get(\"abstract\")\n",
    "            tldr = paper.get(\"tldr\")\n",
    "            if type(doi) == str and len(doi) > 0:\n",
    "                results.append({\n",
    "                    \"title\": paper.get(\"title\",\"\"),\n",
    "                    \"year\": paper.get(\"year\"),\n",
    "                    \"venue\": paper.get(\"venue\",\"\"),\n",
    "                    \"paper_id\": paper.get(\"paperId\"),\n",
    "                    \"doi\": doi,\n",
    "                    \"publication_date\": pub,\n",
    "                    \"oa_pdf_url\": oa,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"tldr\": tldr,\n",
    "                })\n",
    "        token = data.get(\"token\")\n",
    "        if not token: break\n",
    "    return results\n",
    "\n",
    "def get_references(paper_id, fields=\"citedPaper.paperId,citedPaper.externalIds\", max_per_page=1000):\n",
    "    \"\"\"\n",
    "    Robustly fetch references for a paper, returning a list of citedPaper dicts.\n",
    "    Handles 'data': null, pagination via 'next', and non-200 responses.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.semanticscholar.org/graph/v1/paper/{paper_id}/references\"\n",
    "    headers = {\"x-api-key\": SEMANTIC_SCHOLAR_API_KEY} if SEMANTIC_SCHOLAR_API_KEY else {}\n",
    "    params = {\"fields\": fields, \"limit\": max_per_page, \"offset\": 0}\n",
    "    out = []\n",
    "    while True:\n",
    "        r = _get_with_backoff(url, params, headers)\n",
    "        if r.status_code != 200:\n",
    "            # Surface API response text to help debugging auth/rate-limit/etc.\n",
    "            raise RuntimeError(f\"S2 references error {r.status_code}: {r.text[:300]}\")\n",
    "        try:\n",
    "            data = r.json()\n",
    "        except ValueError:\n",
    "            raise RuntimeError(\"S2 references returned non-JSON response\")\n",
    "\n",
    "        items = data.get(\"data\") or []  # <- key fix: 'null' -> []\n",
    "        # Each item normally has {'citedPaper': {...}}; fall back defensively.\n",
    "        for row in items:\n",
    "            cp = (row or {}).get(\"citedPaper\") or row or {}\n",
    "            out.append(cp)\n",
    "\n",
    "        nxt = data.get(\"next\")\n",
    "        if nxt is None:\n",
    "            break\n",
    "        params[\"offset\"] = nxt\n",
    "    return out\n",
    "\n",
    "def get_cited_papers(paper_id: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Return a list of {\"paper_id\": <str>, \"doi\": <str|None>} for all papers\n",
    "    that the given paper_id cites.\n",
    "    \"\"\"\n",
    "    refs = get_references(\n",
    "        paper_id,\n",
    "        fields=\"citedPaper.paperId,citedPaper.externalIds\",\n",
    "        max_per_page=1000\n",
    "    )\n",
    "\n",
    "    out, seen = [], set()\n",
    "    for cp in refs:  # cp is the cited paper dict\n",
    "        pid = cp.get(\"paperId\")\n",
    "        doi = (cp.get(\"externalIds\") or {}).get(\"DOI\")\n",
    "        key = (pid, (doi or \"\").lower())\n",
    "        if pid and key not in seen:\n",
    "            out.append({\"paper_id\": pid, \"doi\": (doi.lower() if doi else None)})\n",
    "            seen.add(key)\n",
    "    return out\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "\n",
    "def _safe_filename_from_doi(doi: str) -> str:\n",
    "    # Make a safe filename from the DOI\n",
    "    # e.g., 10.1038/s41586-019-1049-y -> 10.1038_s41586-019-1049-y.pdf\n",
    "    return re.sub(r'[^A-Za-z0-9._-]+', '_', doi) + \".pdf\"\n",
    "\n",
    "def _extract_text_from_pdf_bytes(pdf_bytes: bytes) -> Optional[str]:\n",
    "    try:\n",
    "        doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "    except Exception:\n",
    "        return None\n",
    "    try:\n",
    "        parts = []\n",
    "        for page in doc:\n",
    "            parts.append(page.get_text())  # plain text extraction\n",
    "        return \"\".join(parts)\n",
    "    finally:\n",
    "        doc.close()\n",
    "\n",
    "def _download_ok(resp: requests.Response) -> bool:\n",
    "    ctype = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
    "    return resp.status_code == 200 and (resp.content and (\"pdf\" in ctype or resp.content[:4] == b\"%PDF\"))\n",
    "\n",
    "# ---------- Main function ----------\n",
    "\n",
    "def fetch_pdf_and_text_by_doi(\n",
    "    doi: str,\n",
    "    save_dir: str = \"/resnick/groups/mthomson/yunruilu/Github_repo/spatial-genomics-llm-collection/PDF\",\n",
    "    oa_pdf_url: Optional[str] = None,\n",
    "    openathens_prefix: str = \"https://go.openathens.net/redirector/caltech.edu?url=\",\n",
    "    session: Optional[requests.Session] = None,\n",
    "    timeout: int = 30,\n",
    ") -> Tuple[Optional[str], Optional[str]]:\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    pdf_bytes = None\n",
    "\n",
    "    # 1) Direct OA URL\n",
    "    if oa_pdf_url:\n",
    "        print(f\"Trying method 1: Direct OA URL for DOI {doi}\")\n",
    "        try:\n",
    "            r = requests.get(oa_pdf_url, timeout=timeout)\n",
    "            if _download_ok(r):\n",
    "                pdf_bytes = r.content\n",
    "                print(f\"Success at method 1: Direct OA URL for DOI {doi}\")\n",
    "            else:\n",
    "                print(f\"Failed method 1: Direct OA URL for DOI {doi}\")\n",
    "        except Exception:\n",
    "            print(f\"Failed method 1: Direct OA URL for DOI {doi}\")\n",
    "\n",
    "    # 2) Unpaywall\n",
    "    if pdf_bytes is None and UNPAYWALL_EMAIL and \"your_email\" not in UNPAYWALL_EMAIL:\n",
    "        print(f\"Trying method 2: Unpaywall for DOI {doi}\")\n",
    "        try:\n",
    "            upw_url = f\"https://api.unpaywall.org/v2/{doi}?email={UNPAYWALL_EMAIL}\"\n",
    "            ur = requests.get(upw_url, timeout=timeout)\n",
    "            if ur.status_code == 200:\n",
    "                j = ur.json()\n",
    "                pdf_url = (j.get(\"best_oa_location\") or {}).get(\"url_for_pdf\") or (j.get(\"best_oa_location\") or {}).get(\"url\")\n",
    "                if not pdf_url:\n",
    "                    for loc in j.get(\"oa_locations\") or []:\n",
    "                        pdf_url = loc.get(\"url_for_pdf\") or loc.get(\"url\")\n",
    "                        if pdf_url: break\n",
    "                if pdf_url:\n",
    "                    pr = requests.get(pdf_url, timeout=timeout)\n",
    "                    if _download_ok(pr):\n",
    "                        pdf_bytes = pr.content\n",
    "                        print(f\"Success at method 2: Unpaywall for DOI {doi}\")\n",
    "                    else:\n",
    "                        print(f\"Failed method 2: Unpaywall for DOI {doi}\")\n",
    "                else:\n",
    "                    print(f\"Failed method 2: Unpaywall for DOI {doi} (no PDF URL found)\")\n",
    "            else:\n",
    "                print(f\"Failed method 2: Unpaywall for DOI {doi} (API error)\")\n",
    "        except Exception:\n",
    "            print(f\"Failed method 2: Unpaywall for DOI {doi}\")\n",
    "\n",
    "    # 3) DOI content negotiation\n",
    "    if pdf_bytes is None:\n",
    "        print(f\"Trying method 3: DOI content negotiation for DOI {doi}\")\n",
    "        try:\n",
    "            r = requests.get(f\"https://doi.org/{doi}\", headers={\"Accept\": \"application/pdf\"}, timeout=timeout, allow_redirects=True)\n",
    "            if _download_ok(r):\n",
    "                pdf_bytes = r.content\n",
    "                print(f\"Success at method 3: DOI content negotiation for DOI {doi}\")\n",
    "            else:\n",
    "                print(f\"Failed method 3: DOI content negotiation for DOI {doi}\")\n",
    "        except Exception:\n",
    "            print(f\"Failed method 3: DOI content negotiation for DOI {doi}\")\n",
    "\n",
    "    # 4) OpenAthens redirector\n",
    "    if pdf_bytes is None and openathens_prefix:\n",
    "        print(f\"Trying method 4: OpenAthens redirector for DOI {doi}\")\n",
    "        try:\n",
    "            sess = session or requests.Session()\n",
    "            proxied_url = f\"{openathens_prefix}{urllib.parse.quote('https://doi.org/' + doi, safe='')}\"\n",
    "            resp = sess.get(proxied_url, allow_redirects=True, timeout=timeout)\n",
    "            ctype = resp.headers.get(\"Content-Type\", \"\")\n",
    "\n",
    "            if ctype.startswith(\"application/pdf\"):\n",
    "                # Got the PDF directly\n",
    "                pdf_bytes = resp.content\n",
    "                print(f\"Success at method 4: OpenAthens redirector for DOI {doi}\")\n",
    "            elif \"html\" in ctype:\n",
    "                html = resp.text\n",
    "                # Check if this is a login page or the article page by looking for clues\n",
    "                if \"openathens.net\" in html.lower() or \"login\" in resp.url:\n",
    "                    print(f\"Failed method 4: OpenAthens redirector for DOI {doi} (authentication required)\")\n",
    "                else:\n",
    "                    # Assume this is the article page HTML, try to find a PDF link\n",
    "                    match = re.search(r'href=\"([^\"]+\\.pdf[^\"]*)\"', html)\n",
    "                    if match:\n",
    "                        pdf_link = match.group(1)\n",
    "                        # Complete relative link if needed\n",
    "                        if pdf_link.startswith(\"/\"):\n",
    "                            from urllib.parse import urljoin\n",
    "                            pdf_link = urljoin(resp.url, pdf_link)\n",
    "                        pdf_resp = sess.get(pdf_link, timeout=timeout)\n",
    "                        if pdf_resp.headers.get(\"Content-Type\",\"\").startswith(\"application/pdf\"):\n",
    "                            pdf_bytes = pdf_resp.content\n",
    "                            print(f\"Success at method 4: OpenAthens redirector for DOI {doi}\")\n",
    "                        else:\n",
    "                            print(f\"Failed method 4: OpenAthens redirector for DOI {doi} (PDF link didn't return PDF)\")\n",
    "                    else:\n",
    "                        print(f\"Failed method 4: OpenAthens redirector for DOI {doi} (no PDF link found on page)\")\n",
    "            else:\n",
    "                print(f\"Failed method 4: OpenAthens redirector for DOI {doi} (unexpected content type)\")\n",
    "        except Exception:\n",
    "            print(f\"Failed method 4: OpenAthens redirector for DOI {doi}\")\n",
    "\n",
    "    if pdf_bytes is None:\n",
    "        print(f\"All methods failed for DOI {doi}\")\n",
    "        return (None, None)\n",
    "\n",
    "    # Save + extract\n",
    "    pdf_name = _safe_filename_from_doi(doi)\n",
    "    dest = os.path.join(save_dir, pdf_name)\n",
    "    with open(dest, \"wb\") as f:\n",
    "        f.write(pdf_bytes)\n",
    "    text = _extract_text_from_pdf_bytes(pdf_bytes) or \"\"\n",
    "    print(f\"Successfully saved PDF and extracted text for DOI {doi}\")\n",
    "    return (dest, text)\n",
    "\n",
    "# Initialize OpenAI client with API key\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def extract_datasets_from_text(full_text: str = None, pdf_path: str = None, paper_title: str = None):\n",
    "    \"\"\"\n",
    "    Extract detailed dataset information from a research paper given its text or PDF file path.\n",
    "    \n",
    "    Either `full_text` or `pdf_path` must be provided. If both are provided, `pdf_path` is prioritized.\n",
    "    \n",
    "    Returns:\n",
    "        A list of dictionaries, each containing details about a dataset used in the paper:\n",
    "        [\n",
    "            {\n",
    "                \"data link\": str,        # Direct URL or DOI link to the dataset if available\n",
    "                \"repository\": str,       # Repository name (e.g., GEO, SRA, Zenodo) or 'Not available'\n",
    "                \"accession\": str,        # Accession ID or DOI of the dataset (if applicable)\n",
    "                \"platform\": str,         # Technology platform (e.g., Visium, Xenium, MERFISH, scRNA-seq, CODEX, etc.)\n",
    "                \"species\": str,          # Organism species (if mentioned)\n",
    "                \"tissue\": str,           # Tissue or sample type (if mentioned)\n",
    "                \"raw_data_available\": bool, # True if raw data files are available, False otherwise\n",
    "                \"available\": bool,       # True if the dataset is publicly available, False if restricted/not available\n",
    "                \"description\": str       # Description of the dataset, including platform resolution and origin (generated by this study or from another source)\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if pdf_path is None and full_text is None:\n",
    "        print(\"Error: No input text or PDF path provided.\")\n",
    "        return []\n",
    "    \n",
    "    # Extract text from PDF if a path is provided\n",
    "    text_content = \"\"\n",
    "    if pdf_path is not None:\n",
    "        try:\n",
    "            # Open the PDF and extract all text\n",
    "            doc = fitz.open(pdf_path)\n",
    "            for page in doc:\n",
    "                text_content += page.get_text()\n",
    "            doc.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read PDF file {pdf_path}: {e}\")\n",
    "            return []\n",
    "    else:\n",
    "        text_content = full_text\n",
    "    \n",
    "    if not text_content:\n",
    "        # If text extraction failed or resulted in empty content\n",
    "        print(\"Error: No text content could be extracted from the input.\")\n",
    "        return []\n",
    "    \n",
    "    # Normalize whitespace and remove hyphenation line breaks for better parsing\n",
    "    text_clean = text_content.replace(\"-\\n\", \"\").replace(\"\\n\", \" \")\n",
    "    # Exclude references section to avoid confusion with data DOIs or accessions in references\n",
    "    text_upper = text_clean.upper()\n",
    "    if \"REFERENCES\" in text_upper:\n",
    "        text_body = text_clean[: text_upper.index(\"REFERENCES\")]\n",
    "    else:\n",
    "        text_body = text_clean\n",
    "    \n",
    "    # Prepare the system and user messages for the GPT model\n",
    "    system_msg = (\n",
    "        \"You are an expert assistant extracting dataset information from scientific papers. \"\n",
    "        \"Identify all datasets mentioned in the paper and extract relevant details. \"\n",
    "        \"Include the repository (or source) and accession/ID or DOI for each dataset, the data platform/technology used \"\n",
    "        \"(e.g., 10x Genomics Visium, 10x Xenium, NanoString CosMX, MERFISH, seqFISH, CODEX, or single-cell RNA-seq if applicable), \"\n",
    "        \"the species and tissue, whether raw data is available, whether the dataset is publicly available, and a brief description. \"\n",
    "        \"whether the authors generated the data or reused the data from another source, and a brief description. \"\n",
    "        \"In the description, mention the platform and its resolution (for example, if it's spatial transcriptomics with spot-based or single-cell resolution, or if it's non-spatial single-cell RNA-seq), \"\n",
    "        \"and state whether the dataset was generated in this study or obtained from another source (citing the source or reference if mentioned).\"\n",
    "    )\n",
    "    user_msg = (\n",
    "        \"Extract all datasets (particularly spatially-resolved omics datasets) mentioned in the following text. \"\n",
    "        \"If the paper includes a single-cell RNA-seq dataset (which is non-spatial) for analysis, include it as well and denote it appropriately. \"\n",
    "        \"Return ONLY a valid JSON array of objects, where each object has the keys: \"\n",
    "        \"data link, repository, accession, platform, species, tissue, raw_data_available, available, original_data, description. \"\n",
    "        \"If a dataset is not publicly available (e.g., available upon request or not provided), set repository to \\\"Not available\\\" and available to false. \"\n",
    "        \"Provide no extra commentary or explanation, only the JSON.\\n\\n\"\n",
    "        f\"Text:\\n\\\"\\\"\\\"\\n{text_body}\\n\\\"\\\"\\\"\"\n",
    "    )\n",
    "    \n",
    "    # Call the OpenAI API to get the dataset details in JSON format\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_input,\n",
    "            # model=\"gpt-4.1\",\n",
    "            # model=OPENAI_MODEL,  # e.g., 'gpt-4' or 'gpt-3.5-turbo'\n",
    "            messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                      {\"role\": \"user\",  \"content\": user_msg}],\n",
    "            # temperature=0\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API error: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # The model's answer (should be JSON or contain JSON)\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Helper to parse the JSON from the model's response\n",
    "    def _extract_json(text: str):\n",
    "        # Attempt direct JSON parse\n",
    "        try:\n",
    "            return json.loads(text), None\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Check for JSON in a markdown code block\n",
    "        match = re.search(r\"```(?:json)?\\s*([\\s\\S]+?)```\", text)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group(1)), None\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "        else:\n",
    "            last_err = None\n",
    "        # Fallback: find first JSON object/array in the text\n",
    "        match = re.search(r\"(\\{.*?\\}|\\[.*?\\])\", text, flags=re.DOTALL)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group(1)), None\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "        return None, last_err\n",
    "    \n",
    "    # Parse the JSON content from the model's output\n",
    "    datasets, err = _extract_json(content)\n",
    "    if err or datasets is None:\n",
    "        print(f\"Failed to parse JSON from model output: {err or 'No JSON found'}\")\n",
    "        return []\n",
    "    \n",
    "    # Ensure the result is a list of dicts\n",
    "    if isinstance(datasets, dict):\n",
    "        datasets = [datasets]\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "\n",
    "\n",
    "result = search_new_papers_bulk(since_days = since_days_input)\n",
    "print(f'Found {len(result)} new papers in the last {since_days_input} days since today: {datetime.date.today()}')\n",
    "\n",
    "for i, one_paper in tqdm(enumerate(result)):\n",
    "    print('--------------------------------')\n",
    "    print(f\"Processing paper {i+1} of {len(result)}\")\n",
    "\n",
    "    items = get_cited_papers(one_paper['paper_id'])\n",
    "    result[i]['reference'] = items\n",
    "\n",
    "    #############################################################################\n",
    "\n",
    "    def fetch_pdf_and_text_by_doi(\n",
    "        doi: str,\n",
    "        save_dir: str = \"/resnick/groups/mthomson/yunruilu/Github_repo/spatial-genomics-llm-collection/PDF\",\n",
    "        oa_pdf_url: Optional[str] = None,\n",
    "        openathens_prefix: str = \"https://go.openathens.net/redirector/caltech.edu?url=\",\n",
    "        session: Optional[requests.Session] = None,\n",
    "        timeout: int = 30,\n",
    "    ) -> Tuple[Optional[str], Optional[str]]:\n",
    "        Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "        pdf_bytes = None\n",
    "\n",
    "        # 1) Direct OA URL\n",
    "        if oa_pdf_url:\n",
    "            if print_details_input:\n",
    "                print(f\"Trying method 1: Direct OA URL for DOI {doi}\")\n",
    "            try:\n",
    "                r = requests.get(oa_pdf_url, timeout=timeout)\n",
    "                if _download_ok(r):\n",
    "                    pdf_bytes = r.content\n",
    "                    if print_details_input:\n",
    "                        print(f\"Success at method 1: Direct OA URL for DOI {doi}\")\n",
    "                else:\n",
    "                    if print_details_input:\n",
    "                        print(f\"Failed method 1: Direct OA URL for DOI {doi}\")\n",
    "            except Exception:\n",
    "                if print_details_input:\n",
    "                    print(f\"Failed method 1: Direct OA URL for DOI {doi}\")\n",
    "\n",
    "        # 2) Unpaywall\n",
    "        if pdf_bytes is None and UNPAYWALL_EMAIL and \"your_email\" not in UNPAYWALL_EMAIL:\n",
    "            if print_details_input:\n",
    "                print(f\"Trying method 2: Unpaywall for DOI {doi}\")\n",
    "            try:\n",
    "                upw_url = f\"https://api.unpaywall.org/v2/{doi}?email={UNPAYWALL_EMAIL}\"\n",
    "                ur = requests.get(upw_url, timeout=timeout)\n",
    "                if ur.status_code == 200:\n",
    "                    j = ur.json()\n",
    "                    pdf_url = (j.get(\"best_oa_location\") or {}).get(\"url_for_pdf\") or (j.get(\"best_oa_location\") or {}).get(\"url\")\n",
    "                    if not pdf_url:\n",
    "                        for loc in j.get(\"oa_locations\") or []:\n",
    "                            pdf_url = loc.get(\"url_for_pdf\") or loc.get(\"url\")\n",
    "                            if pdf_url: break\n",
    "                    if pdf_url:\n",
    "                        pr = requests.get(pdf_url, timeout=timeout)\n",
    "                        if _download_ok(pr):\n",
    "                            pdf_bytes = pr.content\n",
    "                            if print_details_input:\n",
    "                                print(f\"Success at method 2: Unpaywall for DOI {doi}\")\n",
    "                        else:\n",
    "                            if print_details_input:\n",
    "                                print(f\"Failed method 2: Unpaywall for DOI {doi}\")\n",
    "                    else:\n",
    "                        if print_details_input:\n",
    "                            print(f\"Failed method 2: Unpaywall for DOI {doi} (no PDF URL found)\")\n",
    "                else:\n",
    "                    if print_details_input:\n",
    "                        print(f\"Failed method 2: Unpaywall for DOI {doi} (API error)\")\n",
    "            except Exception:\n",
    "                if print_details_input:\n",
    "                    print(f\"Failed method 2: Unpaywall for DOI {doi}\")\n",
    "\n",
    "        # 3) DOI content negotiation\n",
    "        if pdf_bytes is None:\n",
    "            if print_details_input:\n",
    "                print(f\"Trying method 3: DOI content negotiation for DOI {doi}\")\n",
    "            try:\n",
    "                r = requests.get(f\"https://doi.org/{doi}\", headers={\"Accept\": \"application/pdf\"}, timeout=timeout, allow_redirects=True)\n",
    "                if _download_ok(r):\n",
    "                    pdf_bytes = r.content\n",
    "                    if print_details_input:\n",
    "                        print(f\"Success at method 3: DOI content negotiation for DOI {doi}\")\n",
    "                else:\n",
    "                    if print_details_input:\n",
    "                        print(f\"Failed method 3: DOI content negotiation for DOI {doi}\")\n",
    "            except Exception:\n",
    "                if print_details_input:\n",
    "                    print(f\"Failed method 3: DOI content negotiation for DOI {doi}\")\n",
    "\n",
    "        # 4) OpenAthens redirector\n",
    "        if pdf_bytes is None and openathens_prefix:\n",
    "            if print_details_input:\n",
    "                print(f\"Trying method 4: OpenAthens redirector for DOI {doi}\")\n",
    "            try:\n",
    "                sess = session or requests.Session()\n",
    "                proxied_url = f\"{openathens_prefix}{urllib.parse.quote('https://doi.org/' + doi, safe='')}\"\n",
    "                resp = sess.get(proxied_url, allow_redirects=True, timeout=timeout)\n",
    "                ctype = resp.headers.get(\"Content-Type\", \"\")\n",
    "\n",
    "                if ctype.startswith(\"application/pdf\"):\n",
    "                    # Got the PDF directly\n",
    "                    pdf_bytes = resp.content\n",
    "                    if print_details_input:\n",
    "                        print(f\"Success at method 4: OpenAthens redirector for DOI {doi}\")\n",
    "                elif \"html\" in ctype:\n",
    "                    html = resp.text\n",
    "                    # Check if this is a login page or the article page by looking for clues\n",
    "                    if \"openathens.net\" in html.lower() or \"login\" in resp.url:\n",
    "                        if print_details_input:\n",
    "                            print(f\"Failed method 4: OpenAthens redirector for DOI {doi} (authentication required)\")\n",
    "                    else:\n",
    "                        # Assume this is the article page HTML, try to find a PDF link\n",
    "                        match = re.search(r'href=\"([^\"]+\\.pdf[^\"]*)\"', html)\n",
    "                        if match:\n",
    "                            pdf_link = match.group(1)\n",
    "                            # Complete relative link if needed\n",
    "                            if pdf_link.startswith(\"/\"):\n",
    "                                from urllib.parse import urljoin\n",
    "                                pdf_link = urljoin(resp.url, pdf_link)\n",
    "                            pdf_resp = sess.get(pdf_link, timeout=timeout)\n",
    "                            if pdf_resp.headers.get(\"Content-Type\",\"\").startswith(\"application/pdf\"):\n",
    "                                pdf_bytes = pdf_resp.content\n",
    "                                if print_details_input:\n",
    "                                    print(f\"Success at method 4: OpenAthens redirector for DOI {doi}\")\n",
    "                            else:\n",
    "                                if print_details_input:\n",
    "                                    print(f\"Failed method 4: OpenAthens redirector for DOI {doi} (PDF link didn't return PDF)\")\n",
    "                        else:\n",
    "                            if print_details_input:\n",
    "                                print(f\"Failed method 4: OpenAthens redirector for DOI {doi} (no PDF link found on page)\")\n",
    "                else:\n",
    "                    if print_details_input:\n",
    "                        print(f\"Failed method 4: OpenAthens redirector for DOI {doi} (unexpected content type)\")\n",
    "            except Exception:\n",
    "                if print_details_input:\n",
    "                    print(f\"Failed method 4: OpenAthens redirector for DOI {doi}\")\n",
    "\n",
    "        if pdf_bytes is None:\n",
    "            print(f\"All methods failed for DOI {doi}\")\n",
    "            return (None, None)\n",
    "\n",
    "    #############################################################################\n",
    "    path, full_text = fetch_pdf_and_text_by_doi(doi = one_paper['doi'])\n",
    "    if full_text:\n",
    "        data_info = extract_datasets_from_text(full_text = full_text)\n",
    "        if data_info:\n",
    "            if print_details_input:\n",
    "                print(f\"Successfully extracted datasets information\")\n",
    "                for ds in data_info:\n",
    "                    print(json.dumps(ds, indent=2))\n",
    "            result[i]['Datasets_info'] = data_info\n",
    "        else:\n",
    "            result[i]['Datasets_info'] = None\n",
    "    else:\n",
    "        result[i]['Datasets_info'] = None\n",
    "\n",
    "csv_path = \"/resnick/groups/mthomson/yunruilu/Github_repo/spatial-genomics-llm-collection/Papers.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "new_papers_df = pd.DataFrame(result)\n",
    "df_updated = pd.concat([df, new_papers_df], ignore_index=True)\n",
    "df_updated = df_updated.drop_duplicates(subset=['paper_id'], keep='first')\n",
    "\n",
    "df_updated.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Added {len(new_papers_df)} new papers to the CSV file\")\n",
    "print(f\"Total papers in CSV after deduplication: {len(df_updated)}\")\n",
    "# print(display(df_updated.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a96ae952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying method 2: Unpaywall for DOI 10.1101/2025.09.16.25333605\n",
      "Failed method 2: Unpaywall for DOI 10.1101/2025.09.16.25333605 (API error)\n",
      "Trying method 3: DOI content negotiation for DOI 10.1101/2025.09.16.25333605\n",
      "Failed method 3: DOI content negotiation for DOI 10.1101/2025.09.16.25333605\n",
      "Trying method 4: OpenAthens redirector for DOI 10.1101/2025.09.16.25333605\n",
      "Success at method 4: OpenAthens redirector for DOI 10.1101/2025.09.16.25333605\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m path, full_text \u001b[38;5;241m=\u001b[39m fetch_pdf_and_text_by_doi(doi \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10.1101/2025.09.16.25333605\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "path, full_text = fetch_pdf_and_text_by_doi(doi = '10.1101/2025.09.16.25333605')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4d67d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_OPENATHENS = bool(int(os.getenv(\"USE_OPENATHENS\", \"1\")))\n",
    "USE_OPENATHENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "630b85ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from http.cookiejar import LWPCookieJar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ac1aa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = requests.Session()\n",
    "sess.headers.update({\"User-Agent\": \"Mozilla/5.0\"})\n",
    "cookie_path = os.path.expanduser(\"~/.oa_cookies.lwp\")\n",
    "sess.cookies = LWPCookieJar(cookie_path)\n",
    "try: sess.cookies.load(ignore_discard=True, ignore_expires=True)\n",
    "except FileNotFoundError: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "925d89d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP: 131.215.148.41\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "print(\"IP:\", requests.get(\"https://api.ipify.org\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fd2d9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>venue</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>doi</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>oa_pdf_url</th>\n",
       "      <th>abstract</th>\n",
       "      <th>reference</th>\n",
       "      <th>datasets_info</th>\n",
       "      <th>index_paper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cancer-associated fibroblasts drive lung adeno...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Oncogene</td>\n",
       "      <td>13c0faec4d04ddee86bb924ee79c08f0dc4ced64</td>\n",
       "      <td>10.1038/s41388-025-03569-9</td>\n",
       "      <td>2025-09-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mouse-Specific Single cell cytokine activity p...</td>\n",
       "      <td>2025</td>\n",
       "      <td>PLoS Computational Biology</td>\n",
       "      <td>0d3941790d9380f36a851362e4ab7e9d644104dd</td>\n",
       "      <td>10.1371/journal.pcbi.1013475</td>\n",
       "      <td>2025-09-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spatial transcriptomics of intraductal carcino...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Histopathology</td>\n",
       "      <td>9673ebf76c2e0eb9d6b742f4f649f0982e5e6c82</td>\n",
       "      <td>10.1111/his.15551</td>\n",
       "      <td>2025-09-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anatomic Predilection of IDH-Mutant Gliomas: A...</td>\n",
       "      <td>2025</td>\n",
       "      <td>medRxiv</td>\n",
       "      <td>3eb894538844ac916ecd4f91bf9b3d65f8e183cf</td>\n",
       "      <td>10.1101/2025.09.16.25333605</td>\n",
       "      <td>2025-09-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brain Functional-Structural Gradient Coupling ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>medRxiv</td>\n",
       "      <td>377d977c7b456afd9b806d642bc46c324ab53681</td>\n",
       "      <td>10.1101/2025.09.16.25335918</td>\n",
       "      <td>2025-09-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  year  \\\n",
       "0  Cancer-associated fibroblasts drive lung adeno...  2025   \n",
       "1  Mouse-Specific Single cell cytokine activity p...  2025   \n",
       "2  Spatial transcriptomics of intraductal carcino...  2025   \n",
       "3  Anatomic Predilection of IDH-Mutant Gliomas: A...  2025   \n",
       "4  Brain Functional-Structural Gradient Coupling ...  2025   \n",
       "\n",
       "                        venue                                  paper_id  \\\n",
       "0                    Oncogene  13c0faec4d04ddee86bb924ee79c08f0dc4ced64   \n",
       "1  PLoS Computational Biology  0d3941790d9380f36a851362e4ab7e9d644104dd   \n",
       "2              Histopathology  9673ebf76c2e0eb9d6b742f4f649f0982e5e6c82   \n",
       "3                     medRxiv  3eb894538844ac916ecd4f91bf9b3d65f8e183cf   \n",
       "4                     medRxiv  377d977c7b456afd9b806d642bc46c324ab53681   \n",
       "\n",
       "                            doi publication_date  oa_pdf_url abstract  \\\n",
       "0    10.1038/s41388-025-03569-9       2025-09-19         NaN      NaN   \n",
       "1  10.1371/journal.pcbi.1013475       2025-09-19         NaN      NaN   \n",
       "2             10.1111/his.15551       2025-09-18         NaN      NaN   \n",
       "3   10.1101/2025.09.16.25333605       2025-09-18         NaN      NaN   \n",
       "4   10.1101/2025.09.16.25335918       2025-09-18         NaN      NaN   \n",
       "\n",
       "  reference  datasets_info  index_paper  \n",
       "0        []            NaN            0  \n",
       "1        []            NaN            1  \n",
       "2        []            NaN            2  \n",
       "3        []            NaN            3  \n",
       "4        []            NaN            4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "csv_path = \"/resnick/groups/mthomson/yunruilu/Github_repo/spatial-genomics-llm-collection/temp.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80db761d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.1038/s41388-025-03569-9'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['index_paper'] == 0, 'doi'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "016e5435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>venue</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>doi</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>oa_pdf_url</th>\n",
       "      <th>abstract</th>\n",
       "      <th>reference</th>\n",
       "      <th>datasets_info</th>\n",
       "      <th>index_paper</th>\n",
       "      <th>Datasets_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cancer-associated fibroblasts drive lung adeno...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Oncogene</td>\n",
       "      <td>13c0faec4d04ddee86bb924ee79c08f0dc4ced64</td>\n",
       "      <td>10.1038/s41388-025-03569-9</td>\n",
       "      <td>2025-09-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[{\"data link\": \"https://www.ncbi.nlm.nih.gov/g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mouse-Specific Single cell cytokine activity p...</td>\n",
       "      <td>2025</td>\n",
       "      <td>PLoS Computational Biology</td>\n",
       "      <td>0d3941790d9380f36a851362e4ab7e9d644104dd</td>\n",
       "      <td>10.1371/journal.pcbi.1013475</td>\n",
       "      <td>2025-09-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spatial transcriptomics of intraductal carcino...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Histopathology</td>\n",
       "      <td>9673ebf76c2e0eb9d6b742f4f649f0982e5e6c82</td>\n",
       "      <td>10.1111/his.15551</td>\n",
       "      <td>2025-09-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anatomic Predilection of IDH-Mutant Gliomas: A...</td>\n",
       "      <td>2025</td>\n",
       "      <td>medRxiv</td>\n",
       "      <td>3eb894538844ac916ecd4f91bf9b3d65f8e183cf</td>\n",
       "      <td>10.1101/2025.09.16.25333605</td>\n",
       "      <td>2025-09-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>[{\"data link\": \"Not available\", \"repository\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brain Functional-Structural Gradient Coupling ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>medRxiv</td>\n",
       "      <td>377d977c7b456afd9b806d642bc46c324ab53681</td>\n",
       "      <td>10.1101/2025.09.16.25335918</td>\n",
       "      <td>2025-09-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>[{\"data link\": \"https://nda.nih.gov/abcd\", \"re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  year  \\\n",
       "0  Cancer-associated fibroblasts drive lung adeno...  2025   \n",
       "1  Mouse-Specific Single cell cytokine activity p...  2025   \n",
       "2  Spatial transcriptomics of intraductal carcino...  2025   \n",
       "3  Anatomic Predilection of IDH-Mutant Gliomas: A...  2025   \n",
       "4  Brain Functional-Structural Gradient Coupling ...  2025   \n",
       "\n",
       "                        venue                                  paper_id  \\\n",
       "0                    Oncogene  13c0faec4d04ddee86bb924ee79c08f0dc4ced64   \n",
       "1  PLoS Computational Biology  0d3941790d9380f36a851362e4ab7e9d644104dd   \n",
       "2              Histopathology  9673ebf76c2e0eb9d6b742f4f649f0982e5e6c82   \n",
       "3                     medRxiv  3eb894538844ac916ecd4f91bf9b3d65f8e183cf   \n",
       "4                     medRxiv  377d977c7b456afd9b806d642bc46c324ab53681   \n",
       "\n",
       "                            doi publication_date  oa_pdf_url abstract  \\\n",
       "0    10.1038/s41388-025-03569-9       2025-09-19         NaN      NaN   \n",
       "1  10.1371/journal.pcbi.1013475       2025-09-19         NaN      NaN   \n",
       "2             10.1111/his.15551       2025-09-18         NaN      NaN   \n",
       "3   10.1101/2025.09.16.25333605       2025-09-18         NaN      NaN   \n",
       "4   10.1101/2025.09.16.25335918       2025-09-18         NaN      NaN   \n",
       "\n",
       "  reference  datasets_info  index_paper  \\\n",
       "0        []            NaN            0   \n",
       "1        []            NaN            1   \n",
       "2        []            NaN            2   \n",
       "3        []            NaN            3   \n",
       "4        []            NaN            4   \n",
       "\n",
       "                                       Datasets_info  \n",
       "0  [{\"data link\": \"https://www.ncbi.nlm.nih.gov/g...  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3  [{\"data link\": \"Not available\", \"repository\": ...  \n",
       "4  [{\"data link\": \"https://nda.nih.gov/abcd\", \"re...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>venue</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>doi</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>oa_pdf_url</th>\n",
       "      <th>abstract</th>\n",
       "      <th>tldr</th>\n",
       "      <th>reference</th>\n",
       "      <th>datasets_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, year, venue, paper_id, doi, publication_date, oa_pdf_url, abstract, tldr, reference, datasets_info]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = \"/resnick/groups/mthomson/yunruilu/Github_repo/spatial-genomics-llm-collection/temp/temp.csv\"\n",
    "temp_df = pd.read_csv(csv_path)\n",
    "print(display(temp_df.head()))\n",
    "paper_df = pd.read_csv(\"/resnick/groups/mthomson/yunruilu/Github_repo/spatial-genomics-llm-collection/Papers.csv\")\n",
    "print(display(paper_df.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c42029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataframe:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>venue</th>\n",
       "      <th>paper_id</th>\n",
       "      <th>doi</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>oa_pdf_url</th>\n",
       "      <th>abstract</th>\n",
       "      <th>tldr</th>\n",
       "      <th>reference</th>\n",
       "      <th>datasets_info</th>\n",
       "      <th>Datasets_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Extracellular matrix-myCAF signatures correlat...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Clinical Cancer Research</td>\n",
       "      <td>ad25d367f982d21a0fae2317f0a87a9c79d57bcd</td>\n",
       "      <td>10.1158/1078-0432.CCR-25-1098</td>\n",
       "      <td>2025-09-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BACKGROUND\\nImmune checkpoint inhibitors (ICI)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Spatiomolecular mapping reveals anatomical org...</td>\n",
       "      <td>2025</td>\n",
       "      <td>bioRxiv</td>\n",
       "      <td>9a98aaf5505c988d063d10aaeef79ce8d83c81e9</td>\n",
       "      <td>10.1101/2025.09.10.675374</td>\n",
       "      <td>2025-09-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abstract The nucleus accumbens (NAc) is a key ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>S3R: Spatially Smooth and Sparse Regression Re...</td>\n",
       "      <td>2025</td>\n",
       "      <td>bioRxiv</td>\n",
       "      <td>4c8193af35112ddec17374a450d4d9f71fdc898b</td>\n",
       "      <td>10.1101/2025.09.06.674629</td>\n",
       "      <td>2025-09-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Spatial transcriptomics (ST) data demands mode...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'paper_id': '386d4820b21aed3495f0921667564f8...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Gut microbiota shape gene regulatory networks ...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Insect Science</td>\n",
       "      <td>49e6fcee048f716117a2e28478d99c3730300552</td>\n",
       "      <td>10.1111/1744-7917.70157</td>\n",
       "      <td>2025-09-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Honeybees are key pollinators of flowering pla...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>JCHAIN: A Prognostic Marker Based on Pan-Cance...</td>\n",
       "      <td>2025</td>\n",
       "      <td>Genes</td>\n",
       "      <td>4326be73fcb6332128a7cae7d678faa6a3a4870c</td>\n",
       "      <td>10.3390/genes16091070</td>\n",
       "      <td>2025-09-11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Background/Objectives: The JCHAIN (immunoglobu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  year  \\\n",
       "30  Extracellular matrix-myCAF signatures correlat...  2025   \n",
       "31  Spatiomolecular mapping reveals anatomical org...  2025   \n",
       "32  S3R: Spatially Smooth and Sparse Regression Re...  2025   \n",
       "33  Gut microbiota shape gene regulatory networks ...  2025   \n",
       "34  JCHAIN: A Prognostic Marker Based on Pan-Cance...  2025   \n",
       "\n",
       "                       venue                                  paper_id  \\\n",
       "30  Clinical Cancer Research  ad25d367f982d21a0fae2317f0a87a9c79d57bcd   \n",
       "31                   bioRxiv  9a98aaf5505c988d063d10aaeef79ce8d83c81e9   \n",
       "32                   bioRxiv  4c8193af35112ddec17374a450d4d9f71fdc898b   \n",
       "33            Insect Science  49e6fcee048f716117a2e28478d99c3730300552   \n",
       "34                     Genes  4326be73fcb6332128a7cae7d678faa6a3a4870c   \n",
       "\n",
       "                              doi publication_date oa_pdf_url  \\\n",
       "30  10.1158/1078-0432.CCR-25-1098       2025-09-11        NaN   \n",
       "31      10.1101/2025.09.10.675374       2025-09-11        NaN   \n",
       "32      10.1101/2025.09.06.674629       2025-09-11        NaN   \n",
       "33        10.1111/1744-7917.70157       2025-09-11        NaN   \n",
       "34          10.3390/genes16091070       2025-09-11        NaN   \n",
       "\n",
       "                                             abstract tldr  \\\n",
       "30  BACKGROUND\\nImmune checkpoint inhibitors (ICI)...  NaN   \n",
       "31  Abstract The nucleus accumbens (NAc) is a key ...  NaN   \n",
       "32  Spatial transcriptomics (ST) data demands mode...  NaN   \n",
       "33  Honeybees are key pollinators of flowering pla...  NaN   \n",
       "34  Background/Objectives: The JCHAIN (immunoglobu...  NaN   \n",
       "\n",
       "                                            reference datasets_info  \\\n",
       "30                                                 []           NaN   \n",
       "31                                                 []           NaN   \n",
       "32  [{'paper_id': '386d4820b21aed3495f0921667564f8...           NaN   \n",
       "33                                                 []           NaN   \n",
       "34                                                 []           NaN   \n",
       "\n",
       "   Datasets_info  \n",
       "30           NaN  \n",
       "31           NaN  \n",
       "32           NaN  \n",
       "33           NaN  \n",
       "34           NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Drop the index_paper column from temp_df\n",
    "temp_df = temp_df.drop(columns=['index_paper'], errors='ignore')\n",
    "\n",
    "# Append temp_df to paper_df, ensuring columns match\n",
    "paper_df = pd.concat([paper_df, temp_df], ignore_index=True, sort=False)\n",
    "\n",
    "# # Display the combined dataframe\n",
    "# print(\"Combined dataframe:\")\n",
    "# display(paper_df.tail())\n",
    "# Remove duplicates based on paper_id, keeping the first row where Datasets_info is not None\n",
    "# First, sort by paper_id and put non-null Datasets_info rows first\n",
    "paper_df_sorted = paper_df.sort_values(\n",
    "    by=['paper_id', 'Datasets_info'], \n",
    "    key=lambda x: x.isnull() if x.name == 'Datasets_info' else x,\n",
    "    na_position='last'\n",
    ")\n",
    "\n",
    "# Drop duplicates keeping the first occurrence (which will have non-null Datasets_info if available)\n",
    "paper_df = paper_df_sorted.drop_duplicates(subset=['paper_id'], keep='first')\n",
    "\n",
    "# Reset index\n",
    "paper_df = paper_df.reset_index(drop=True)\n",
    "\n",
    "# print(\"After removing duplicates:\")\n",
    "# display(paper_df.tail())\n",
    "\n",
    "paper_df.to_csv(\"/resnick/groups/mthomson/yunruilu/Github_repo/spatial-genomics-llm-collection/Papers.csv\", index=False)\n",
    "print('Papers.csv updated')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paper_Collection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
