{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b2dc159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# # Parse command line arguments\n",
    "# parser = argparse.ArgumentParser(description='Search and analyze spatial genomics papers')\n",
    "# parser.add_argument('--since_days', type=int, default=10, help='Number of days to search back')\n",
    "# parser.add_argument('--print_details', type=str, default='True', help='Whether to print details (True/False)')\n",
    "# parser.add_argument('--model', type=str, default='gpt-5', help='OpenAI model to use')\n",
    "# parser.add_argument('--search_query', type=str, default='spatial transcriptomics', help='Search query', nargs='+')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# # Convert string to boolean for print_details\n",
    "# print_details_input = args.print_details.lower() in ['true', '1', 'yes', 'on']\n",
    "# since_days_input = args.since_days\n",
    "# model_input = args.model\n",
    "# search_query_input = ' '.join(args.search_query)\n",
    "print_details_input = True\n",
    "since_days_input = 10\n",
    "model_input = 'gpt-5'\n",
    "search_query_input = 'spatial transcriptomics'\n",
    "\n",
    "SEMANTIC_SCHOLAR_API_KEY = \"19tQFoyv7w5xBQNMsUA7C5lwNqEni5g3GKkP8Pkj\"  # e.g., 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' | https://www.semanticscholar.org/product/api/tutorial?utm_campaign=API%20transaction&utm_medium=email&_hsenc=p2ANqtz--KbD5dVfVRom22kVjKkL-55Ikb73h1Nze5JYW6_8OGfj15Pf_Z7OjRXzHnO2BntuA89mE6jdPyHOEzQnaYDLInKFPGxw&_hsmi=329822401&utm_content=329822401&utm_source=hs_automation\n",
    "OPENAI_API_KEY = \"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\"  # e.g., 'sk-...'\n",
    "UNPAYWALL_EMAIL = \"yunruilu@caltech.edu\"  # Email for Unpaywall API\n",
    "NCBI_EMAIL = \"yunruilu@caltech.edu\"  # Email for NCBI Entrez (required by NCBI)\n",
    "NCBI_API_KEY = '903b8602ced7c96ae73650c3ff78350a100'  # Optional: NCBI API Key for higher rate limits, or None | https://support.nlm.nih.gov/kbArticle/?pn=KA-05317\n",
    "USE_OPENATHENS = bool(int(os.getenv(\"USE_OPENATHENS\", \"1\")))\n",
    "OPENATHENS_PREFIX = \"https://go.openathens.net/redirector/caltech.edu?url=\"\n",
    "\n",
    "\n",
    "# Model and other options\n",
    "# OPENAI_MODEL = \"gpt-5\"  # Use 'gpt-4' for best results; 'gpt-3.5-turbo' if lower cost is desired\n",
    "INSTITUTIONAL_ACCESS = True  # True if running on a network with institutional access to paywalled PDFs\n",
    "\n",
    "# Search query and settings\n",
    "# SEARCH_QUERY = '(\"spatial transcriptomics\" OR Visium OR MERFISH OR seqFISH OR CosMX OR Xenium)'\n",
    "SEARCH_QUERY = search_query_input\n",
    "# FIELDS_OF_STUDY = \"Biology\"  # Restrict search to biology-related papers\n",
    "FIELDS_OF_STUDY = None\n",
    "\n",
    "# search_papers_bulk.py\n",
    "import requests, datetime, time\n",
    "from tqdm import tqdm\n",
    "# fetch_pdf_pipeline.py\n",
    "import re, io, time, urllib.parse, requests, fitz\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, Tuple\n",
    "import re\n",
    "import json\n",
    "import fitz  # PyMuPDF for PDF parsing\n",
    "from openai import OpenAI\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# from http.cookiejar import LWPCookieJar\n",
    "\n",
    "sess = requests.Session()\n",
    "# sess.headers.update({\"User-Agent\": \"Mozilla/5.0\"})\n",
    "# cookie_path = os.path.expanduser(\"~/.oa_cookies.lwp\")\n",
    "# sess.cookies = LWPCookieJar(cookie_path)\n",
    "# try: sess.cookies.load(ignore_discard=True, ignore_expires=True)\n",
    "# except FileNotFoundError: pass\n",
    "\n",
    "# import subprocess, sys, json, os, shlex\n",
    "# from pathlib import Path\n",
    "\n",
    "# WORKER = Path(__file__).with_name(\"fetch_pdf_worker.py\")  # assumes same folder\n",
    "# OUT_DIR = \"./Data\"  # or wherever you want\n",
    "\n",
    "# def fetch_via_worker(doi: str, oa_pdf_url: Optional[str] = None) -> dict:\n",
    "#     cmd = [\n",
    "#         sys.executable, str(WORKER),\n",
    "#         \"--doi\", doi,\n",
    "#         \"--out-dir\", OUT_DIR,\n",
    "#         \"--cookie-jar\", os.path.expanduser(\"~/.oa_cookies.lwp\"),\n",
    "#         \"--openathens-prefix\", os.getenv(\"OPENATHENS_PREFIX\", \"https://go.openathens.net/redirector/caltech.edu?url=\"),\n",
    "#         \"--unpaywall-email\", os.getenv(\"UNPAYWALL_EMAIL\", \"you@example.edu\"),\n",
    "#         # \"--force\"  # uncomment if you want to re-download\n",
    "#     ]\n",
    "#     if oa_pdf_url:\n",
    "#         cmd += [\"--oa-pdf-url\", oa_pdf_url]\n",
    "\n",
    "#     # Run worker and capture a single JSON line on STDOUT\n",
    "#     proc = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "#     # Optional: print logs to help debugging\n",
    "#     if proc.stderr:\n",
    "#         print(proc.stderr, file=sys.stderr)\n",
    "\n",
    "#     try:\n",
    "#         payload = json.loads(proc.stdout.strip().splitlines()[-1])\n",
    "#     except Exception:\n",
    "#         payload = {\"status\": \"failed\", \"detail\": \"no_json_from_worker\", \"stdout\": proc.stdout}\n",
    "\n",
    "#     payload[\"returncode\"] = proc.returncode\n",
    "#     return payload\n",
    "\n",
    "\n",
    "def _get_with_backoff(url, params, headers, max_retries=5, timeout=30, session=None):\n",
    "    delay = 1.0\n",
    "    for _ in range(max_retries):\n",
    "        s = session or requests.Session()\n",
    "        r = s.get(url, params=params, headers=headers, timeout=timeout)\n",
    "        if r.status_code in (429,) or 500 <= r.status_code < 600:\n",
    "            time.sleep(delay); delay = min(delay * 2, 30); continue\n",
    "        return r\n",
    "    return r\n",
    "\n",
    "def search_new_papers_bulk(since_days=365):\n",
    "    since_date = datetime.date.today() - datetime.timedelta(days=since_days)\n",
    "    url = \"https://api.semanticscholar.org/graph/v1/paper/search/bulk\"\n",
    "\n",
    "    headers = {}\n",
    "    if SEMANTIC_SCHOLAR_API_KEY:\n",
    "        headers[\"x-api-key\"] = SEMANTIC_SCHOLAR_API_KEY\n",
    "\n",
    "    params = {\n",
    "        \"query\": SEARCH_QUERY,\n",
    "        # \"query\": 'spatial transcriptomics',                       # simpler query string\n",
    "        \"fields\": \"title,year,venue,paperId,externalIds,openAccessPdf,publicationDate,abstract\",\n",
    "        \"publicationDateOrYear\": f\"{since_date.isoformat()}:{datetime.date.today().isoformat()}\",\n",
    "        \"sort\": \"publicationDate:desc\",\n",
    "        \"limit\": 1000,\n",
    "        \"fieldsOfStudy\": FIELDS_OF_STUDY,\n",
    "    }\n",
    "\n",
    "    results, token = [], None\n",
    "    while True:\n",
    "        p = params.copy()\n",
    "        if token: p[\"token\"] = token\n",
    "        resp = _get_with_backoff(url, p, headers, session=sess)\n",
    "        if resp.status_code != 200:\n",
    "            print(\"API\", resp.status_code, resp.text[:500]); break\n",
    "\n",
    "        data = resp.json()\n",
    "        for paper in data.get(\"data\", []):\n",
    "            pub = paper.get(\"publicationDate\")\n",
    "            doi = (paper.get(\"externalIds\") or {}).get(\"DOI\")\n",
    "            oa  = (paper.get(\"openAccessPdf\") or {}).get(\"url\")\n",
    "            abstract = paper.get(\"abstract\")\n",
    "            # tldr = paper.get(\"tldr\")\n",
    "            if doi and type(doi) == str and len(doi) > 0:\n",
    "                results.append({\n",
    "                    \"title\": paper.get(\"title\",\"\"),\n",
    "                    \"year\": paper.get(\"year\"),\n",
    "                    \"venue\": paper.get(\"venue\",\"\"),\n",
    "                    \"paper_id\": paper.get(\"paperId\"),\n",
    "                    \"doi\": doi,\n",
    "                    \"publication_date\": pub,\n",
    "                    \"oa_pdf_url\": oa,\n",
    "                    \"abstract\": abstract,\n",
    "                    # \"tldr\": tldr,\n",
    "                })\n",
    "        token = data.get(\"token\")\n",
    "        if not token: break\n",
    "    return results\n",
    "\n",
    "def get_references(paper_id, fields=\"citedPaper.paperId,citedPaper.externalIds\", max_per_page=1000):\n",
    "    \"\"\"\n",
    "    Robustly fetch references for a paper, returning a list of citedPaper dicts.\n",
    "    Handles 'data': null, pagination via 'next', and non-200 responses.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.semanticscholar.org/graph/v1/paper/{paper_id}/references\"\n",
    "    headers = {\"x-api-key\": SEMANTIC_SCHOLAR_API_KEY} if SEMANTIC_SCHOLAR_API_KEY else {}\n",
    "    params = {\"fields\": fields, \"limit\": max_per_page, \"offset\": 0}\n",
    "    out = []\n",
    "    while True:\n",
    "        r = _get_with_backoff(url, params, headers, session=sess)\n",
    "        if r.status_code != 200:\n",
    "            # Surface API response text to help debugging auth/rate-limit/etc.\n",
    "            raise RuntimeError(f\"S2 references error {r.status_code}: {r.text[:300]}\")\n",
    "        try:\n",
    "            data = r.json()\n",
    "        except ValueError:\n",
    "            raise RuntimeError(\"S2 references returned non-JSON response\")\n",
    "\n",
    "        items = data.get(\"data\") or []  # <- key fix: 'null' -> []\n",
    "        # Each item normally has {'citedPaper': {...}}; fall back defensively.\n",
    "        for row in items:\n",
    "            cp = (row or {}).get(\"citedPaper\") or row or {}\n",
    "            out.append(cp)\n",
    "\n",
    "        nxt = data.get(\"next\")\n",
    "        if nxt is None:\n",
    "            break\n",
    "        params[\"offset\"] = nxt\n",
    "    return out\n",
    "\n",
    "def get_cited_papers(paper_id: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Return a list of {\"paper_id\": <str>, \"doi\": <str|None>} for all papers\n",
    "    that the given paper_id cites.\n",
    "    \"\"\"\n",
    "    refs = get_references(\n",
    "        paper_id,\n",
    "        fields=\"citedPaper.paperId,citedPaper.externalIds\",\n",
    "        max_per_page=1000\n",
    "    )\n",
    "\n",
    "    out, seen = [], set()\n",
    "    for cp in refs:  # cp is the cited paper dict\n",
    "        pid = cp.get(\"paperId\")\n",
    "        doi = (cp.get(\"externalIds\") or {}).get(\"DOI\")\n",
    "        key = (pid, (doi or \"\").lower())\n",
    "        if pid and key not in seen:\n",
    "            out.append({\"paper_id\": pid, \"doi\": (doi.lower() if doi else None)})\n",
    "            seen.add(key)\n",
    "    return out\n",
    "\n",
    "# ---------- Small helpers ----------\n",
    "\n",
    "def _safe_filename_from_doi(doi: str) -> str:\n",
    "    # Make a safe filename from the DOI\n",
    "    # e.g., 10.1038/s41586-019-1049-y -> 10.1038_s41586-019-1049-y.pdf\n",
    "    return re.sub(r'[^A-Za-z0-9._-]+', '_', doi) + \".pdf\"\n",
    "\n",
    "def _extract_text_from_pdf_bytes(pdf_bytes: bytes) -> Optional[str]:\n",
    "    try:\n",
    "        doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "    except Exception:\n",
    "        return None\n",
    "    try:\n",
    "        parts = []\n",
    "        for page in doc:\n",
    "            parts.append(page.get_text())  # plain text extraction\n",
    "        return \"\".join(parts)\n",
    "    finally:\n",
    "        doc.close()\n",
    "\n",
    "def _download_ok(resp: requests.Response) -> bool:\n",
    "    ctype = (resp.headers.get(\"Content-Type\") or \"\").lower()\n",
    "    return resp.status_code == 200 and (resp.content and (\"pdf\" in ctype or resp.content[:4] == b\"%PDF\"))\n",
    "\n",
    "# ---------- Main function ----------\n",
    "\n",
    "def fetch_pdf_and_text_by_doi(\n",
    "    doi: str,\n",
    "    save_dir: str = \"/resnick/groups/mthomson/yunruilu/Github_repo/spatial-genomics-llm-collection/PDF\",\n",
    "    oa_pdf_url: Optional[str] = None,\n",
    "    openathens_prefix: str = \"https://go.openathens.net/redirector/caltech.edu?url=\",\n",
    "    session: Optional[requests.Session] = None,\n",
    "    timeout: int = 30,\n",
    ") -> Tuple[Optional[str], Optional[str]]:\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    pdf_bytes = None\n",
    "    s = session or requests.Session()\n",
    "\n",
    "    # 1) Direct OA URL\n",
    "    if oa_pdf_url:\n",
    "        print(f\"Trying method 1: Direct OA URL for DOI {doi}\")\n",
    "        try:\n",
    "            r = s.get(oa_pdf_url, timeout=timeout)\n",
    "            if _download_ok(r):\n",
    "                pdf_bytes = r.content\n",
    "                print(f\"Success at method 1: Direct OA URL for DOI {doi}\")\n",
    "            else:\n",
    "                print(f\"Failed method 1: Direct OA URL for DOI {doi}\")\n",
    "        except Exception:\n",
    "            print(f\"Failed method 1: Direct OA URL for DOI {doi}\")\n",
    "\n",
    "    # 2) Unpaywall\n",
    "    if pdf_bytes is None and UNPAYWALL_EMAIL and \"your_email\" not in UNPAYWALL_EMAIL:\n",
    "        print(f\"Trying method 2: Unpaywall for DOI {doi}\")\n",
    "        try:\n",
    "            upw_url = f\"https://api.unpaywall.org/v2/{doi}?email={UNPAYWALL_EMAIL}\"\n",
    "            ur = s.get(upw_url, timeout=timeout)\n",
    "            if ur.status_code == 200:\n",
    "                j = ur.json()\n",
    "                pdf_url = (j.get(\"best_oa_location\") or {}).get(\"url_for_pdf\") or (j.get(\"best_oa_location\") or {}).get(\"url\")\n",
    "                if not pdf_url:\n",
    "                    for loc in j.get(\"oa_locations\") or []:\n",
    "                        pdf_url = loc.get(\"url_for_pdf\") or loc.get(\"url\")\n",
    "                        if pdf_url: break\n",
    "                if pdf_url:\n",
    "                    pr = s.get(pdf_url, timeout=timeout)\n",
    "                    if _download_ok(pr):\n",
    "                        pdf_bytes = pr.content\n",
    "                        print(f\"Success at method 2: Unpaywall for DOI {doi}\")\n",
    "                    else:\n",
    "                        print(f\"Failed method 2: Unpaywall for DOI {doi}\")\n",
    "                else:\n",
    "                    print(f\"Failed method 2: Unpaywall for DOI {doi} (no PDF URL found)\")\n",
    "            else:\n",
    "                print(f\"Failed method 2: Unpaywall for DOI {doi} (API error)\")\n",
    "        except Exception:\n",
    "            print(f\"Failed method 2: Unpaywall for DOI {doi}\")\n",
    "\n",
    "    # 3) DOI content negotiation\n",
    "    if pdf_bytes is None:\n",
    "        print(f\"Trying method 3: DOI content negotiation for DOI {doi}\")\n",
    "        try:\n",
    "            r = s.get(f\"https://doi.org/{doi}\", headers={\"Accept\": \"application/pdf\"}, timeout=timeout, allow_redirects=True)\n",
    "            if _download_ok(r):\n",
    "                pdf_bytes = r.content\n",
    "                print(f\"Success at method 3: DOI content negotiation for DOI {doi}\")\n",
    "            else:\n",
    "                print(f\"Failed method 3: DOI content negotiation for DOI {doi}\")\n",
    "        except Exception:\n",
    "            print(f\"Failed method 3: DOI content negotiation for DOI {doi}\")\n",
    "\n",
    "    # 4) OpenAthens redirector\n",
    "    if pdf_bytes is None and openathens_prefix:\n",
    "        print(f\"Trying method 4: OpenAthens redirector for DOI {doi}\")\n",
    "        try:\n",
    "            # s = session # or requests.Session()\n",
    "            proxied_url = f\"{openathens_prefix}{urllib.parse.quote('https://doi.org/' + doi, safe='')}\"\n",
    "            resp = s.get(proxied_url, allow_redirects=True, timeout=timeout)\n",
    "            ctype = resp.headers.get(\"Content-Type\", \"\")\n",
    "\n",
    "            if ctype.startswith(\"application/pdf\"):\n",
    "                # Got the PDF directly\n",
    "                pdf_bytes = resp.content\n",
    "                print(f\"Success at method 4: OpenAthens redirector for DOI {doi}\")\n",
    "            elif \"html\" in ctype:\n",
    "                html = resp.text\n",
    "                # Check if this is a login page or the article page by looking for clues\n",
    "                if \"openathens.net\" in html.lower() or \"login\" in resp.url:\n",
    "                    print(f\"Failed method 4: OpenAthens redirector for DOI {doi} (authentication required)\")\n",
    "                else:\n",
    "                    # Assume this is the article page HTML, try to find a PDF link\n",
    "                    match = re.search(r'href=\"([^\"]+\\.pdf[^\"]*)\"', html)\n",
    "                    if match:\n",
    "                        pdf_link = match.group(1)\n",
    "                        # Complete relative link if needed\n",
    "                        if pdf_link.startswith(\"/\"):\n",
    "                            from urllib.parse import urljoin\n",
    "                            pdf_link = urljoin(resp.url, pdf_link)\n",
    "                        pdf_resp = s.get(pdf_link, timeout=timeout)\n",
    "                        if pdf_resp.headers.get(\"Content-Type\",\"\").startswith(\"application/pdf\"):\n",
    "                            pdf_bytes = pdf_resp.content\n",
    "                            print(f\"Success at method 4: OpenAthens redirector for DOI {doi}\")\n",
    "                        else:\n",
    "                            print(f\"Failed method 4: OpenAthens redirector for DOI {doi} (PDF link didn't return PDF)\")\n",
    "                    else:\n",
    "                        print(f\"Failed method 4: OpenAthens redirector for DOI {doi} (no PDF link found on page)\")\n",
    "            else:\n",
    "                print(f\"Failed method 4: OpenAthens redirector for DOI {doi} (unexpected content type)\")\n",
    "        except Exception:\n",
    "            print(f\"Failed method 4: OpenAthens redirector for DOI {doi}\")\n",
    "\n",
    "    if pdf_bytes is None:\n",
    "        print(f\"All methods failed for DOI {doi}\")\n",
    "        return (None, None)\n",
    "\n",
    "    # Save + extract\n",
    "    pdf_name = _safe_filename_from_doi(doi)\n",
    "    dest = os.path.join(save_dir, pdf_name)\n",
    "    with open(dest, \"wb\") as f:\n",
    "        f.write(pdf_bytes)\n",
    "    text = _extract_text_from_pdf_bytes(pdf_bytes) or \"\"\n",
    "    print(f\"Successfully saved PDF and extracted text for DOI {doi}\")\n",
    "    return (dest, text)\n",
    "\n",
    "# Initialize OpenAI client with API key\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def extract_datasets_from_text(full_text: str = None, pdf_path: str = None, paper_title: str = None):\n",
    "    \"\"\"\n",
    "    Extract detailed dataset information from a research paper given its text or PDF file path.\n",
    "    \n",
    "    Either `full_text` or `pdf_path` must be provided. If both are provided, `pdf_path` is prioritized.\n",
    "    \n",
    "    Returns:\n",
    "        A list of dictionaries, each containing details about a dataset used in the paper:\n",
    "        [\n",
    "            {\n",
    "                \"data link\": str,        # Direct URL or DOI link to the dataset if available\n",
    "                \"repository\": str,       # Repository name (e.g., GEO, SRA, Zenodo) or 'Not available'\n",
    "                \"accession\": str,        # Accession ID or DOI of the dataset (if applicable)\n",
    "                \"platform\": str,         # Technology platform (e.g., Visium, Xenium, MERFISH, scRNA-seq, CODEX, etc.)\n",
    "                \"species\": str,          # Organism species (if mentioned)\n",
    "                \"tissue\": str,           # Tissue or sample type (if mentioned)\n",
    "                \"raw_data_available\": bool, # True if raw data files are available, False otherwise\n",
    "                \"available\": bool,       # True if the dataset is publicly available, False if restricted/not available\n",
    "                \"description\": str       # Description of the dataset, including platform resolution and origin (generated by this study or from another source)\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "    \"\"\"\n",
    "    # Validate input\n",
    "    if pdf_path is None and full_text is None:\n",
    "        print(\"Error: No input text or PDF path provided.\")\n",
    "        return []\n",
    "    \n",
    "    # Extract text from PDF if a path is provided\n",
    "    text_content = \"\"\n",
    "    if pdf_path is not None:\n",
    "        try:\n",
    "            # Open the PDF and extract all text\n",
    "            doc = fitz.open(pdf_path)\n",
    "            for page in doc:\n",
    "                text_content += page.get_text()\n",
    "            doc.close()\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read PDF file {pdf_path}: {e}\")\n",
    "            return []\n",
    "    else:\n",
    "        text_content = full_text\n",
    "    \n",
    "    if not text_content:\n",
    "        # If text extraction failed or resulted in empty content\n",
    "        print(\"Error: No text content could be extracted from the input.\")\n",
    "        return []\n",
    "    \n",
    "    # Normalize whitespace and remove hyphenation line breaks for better parsing\n",
    "    text_clean = text_content.replace(\"-\\n\", \"\").replace(\"\\n\", \" \")\n",
    "    # Exclude references section to avoid confusion with data DOIs or accessions in references\n",
    "    text_upper = text_clean.upper()\n",
    "    if \"REFERENCES\" in text_upper:\n",
    "        text_body = text_clean[: text_upper.index(\"REFERENCES\")]\n",
    "    else:\n",
    "        text_body = text_clean\n",
    "    \n",
    "    # Prepare the system and user messages for the GPT model\n",
    "    system_msg = (\n",
    "        \"You are an expert assistant extracting dataset information from scientific papers. \"\n",
    "        \"Identify all datasets mentioned in the paper and extract relevant details. \"\n",
    "        \"Include the repository (or source) and accession/ID or DOI for each dataset, the data platform/technology used \"\n",
    "        \"(e.g., 10x Genomics Visium, 10x Xenium, NanoString CosMX, MERFISH, seqFISH, CODEX, or single-cell RNA-seq if applicable), \"\n",
    "        \"the species and tissue, whether raw data is available, whether the dataset is publicly available, and a brief description. \"\n",
    "        \"whether the authors generated the data or reused the data from another source, and a brief description. \"\n",
    "        \"In the description, mention the platform and its resolution (for example, if it's spatial transcriptomics with spot-based or single-cell resolution, or if it's non-spatial single-cell RNA-seq), \"\n",
    "        \"and state whether the dataset was generated in this study or obtained from another source (citing the source or reference if mentioned).\"\n",
    "    )\n",
    "    user_msg = (\n",
    "        \"Extract all datasets (particularly spatially-resolved omics datasets) mentioned in the following text. \"\n",
    "        \"If the paper includes a single-cell RNA-seq dataset (which is non-spatial) for analysis, include it as well and denote it appropriately. \"\n",
    "        \"Return ONLY a valid JSON array of objects, where each object has the keys: \"\n",
    "        \"data link, repository, accession, platform, species, tissue, raw_data_available, available, original_data, description. \"\n",
    "        \"If a dataset is not publicly available (e.g., available upon request or not provided), set repository to \\\"Not available\\\" and available to false. \"\n",
    "        \"Provide no extra commentary or explanation, only the JSON.\\n\\n\"\n",
    "        f\"Text:\\n\\\"\\\"\\\"\\n{text_body}\\n\\\"\\\"\\\"\"\n",
    "    )\n",
    "    \n",
    "    # Call the OpenAI API to get the dataset details in JSON format\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_input,\n",
    "            # model=\"gpt-4.1\",\n",
    "            # model=OPENAI_MODEL,  # e.g., 'gpt-4' or 'gpt-3.5-turbo'\n",
    "            messages=[{\"role\": \"system\", \"content\": system_msg},\n",
    "                      {\"role\": \"user\",  \"content\": user_msg}],\n",
    "            # temperature=0\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API error: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # The model's answer (should be JSON or contain JSON)\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Helper to parse the JSON from the model's response\n",
    "    def _extract_json(text: str):\n",
    "        # Attempt direct JSON parse\n",
    "        try:\n",
    "            return json.loads(text), None\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Check for JSON in a markdown code block\n",
    "        match = re.search(r\"```(?:json)?\\s*([\\s\\S]+?)```\", text)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group(1)), None\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "        else:\n",
    "            last_err = None\n",
    "        # Fallback: find first JSON object/array in the text\n",
    "        match = re.search(r\"(\\{.*?\\}|\\[.*?\\])\", text, flags=re.DOTALL)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group(1)), None\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "        return None, last_err\n",
    "    \n",
    "    # Parse the JSON content from the model's output\n",
    "    datasets, err = _extract_json(content)\n",
    "    if err or datasets is None:\n",
    "        print(f\"Failed to parse JSON from model output: {err or 'No JSON found'}\")\n",
    "        return []\n",
    "    \n",
    "    # Ensure the result is a list of dicts\n",
    "    if isinstance(datasets, dict):\n",
    "        datasets = [datasets]\n",
    "    \n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce61adb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35 new papers in the last 10 days since today: 2025-09-21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------\n",
      "Processing paper 22 of 35, doi: 10.1038/s41592-025-02773-5\n",
      "Trying method 2: Unpaywall for DOI 10.1038/s41592-025-02773-5\n",
      "Failed method 2: Unpaywall for DOI 10.1038/s41592-025-02773-5 (no PDF URL found)\n",
      "Trying method 3: DOI content negotiation for DOI 10.1038/s41592-025-02773-5\n",
      "Failed method 3: DOI content negotiation for DOI 10.1038/s41592-025-02773-5\n",
      "Trying method 4: OpenAthens redirector for DOI 10.1038/s41592-025-02773-5\n",
      "Success at method 4: OpenAthens redirector for DOI 10.1038/s41592-025-02773-5\n",
      "Successfully saved PDF and extracted text for DOI 10.1038/s41592-025-02773-5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [02:13,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted datasets information\n",
      "{\n",
      "  \"data link\": \"Not available\",\n",
      "  \"repository\": \"Not available\",\n",
      "  \"accession\": \"P5847\",\n",
      "  \"platform\": \"Single-cell RNA-seq (platform unspecified)\",\n",
      "  \"species\": \"Human\",\n",
      "  \"tissue\": \"Primary gastrointestinal tumor\",\n",
      "  \"raw_data_available\": false,\n",
      "  \"available\": false,\n",
      "  \"original_data\": false,\n",
      "  \"description\": \"Non-spatial single-cell RNA-seq from a primary gastrointestinal cancer sample (P5847); used for CNA-based malignant cell labeling and subclone detection; dataset reused from prior sources (no public link provided).\"\n",
      "}\n",
      "{\n",
      "  \"data link\": \"Not available\",\n",
      "  \"repository\": \"Not available\",\n",
      "  \"accession\": \"P5931\",\n",
      "  \"platform\": \"Single-cell RNA-seq (platform unspecified)\",\n",
      "  \"species\": \"Human\",\n",
      "  \"tissue\": \"Primary gastric tumor\",\n",
      "  \"raw_data_available\": false,\n",
      "  \"available\": false,\n",
      "  \"original_data\": false,\n",
      "  \"description\": \"Non-spatial single-cell RNA-seq from a primary gastric tumor (P5931); used to benchmark subclone detection against matched scDNA-seq (ref. 9); dataset reused (no public link provided).\"\n",
      "}\n",
      "{\n",
      "  \"data link\": \"Not available\",\n",
      "  \"repository\": \"Not available\",\n",
      "  \"accession\": \"P8823\",\n",
      "  \"platform\": \"Single-cell RNA-seq (platform unspecified)\",\n",
      "  \"species\": \"Human\",\n",
      "  \"tissue\": \"Primary gastrointestinal tumor\",\n",
      "  \"raw_data_available\": false,\n",
      "  \"available\": false,\n",
      "  \"original_data\": false,\n",
      "  \"description\": \"Non-spatial single-cell RNA-seq from a primary gastrointestinal cancer sample (P8823); used for CNA and subclone analyses; dataset reused (no public link provided).\"\n",
      "}\n",
      "{\n",
      "  \"data link\": \"Not available\",\n",
      "  \"repository\": \"Not available\",\n",
      "  \"accession\": \"P6198\",\n",
      "  \"platform\": \"Single-cell RNA-seq (platform unspecified)\",\n",
      "  \"species\": \"Human\",\n",
      "  \"tissue\": \"Colorectal cancer metastasis\",\n",
      "  \"raw_data_available\": false,\n",
      "  \"available\": false,\n",
      "  \"original_data\": false,\n",
      "  \"description\": \"Non-spatial single-cell RNA-seq from a colorectal metastasis sample (P6198) with expert cell type annotations (ref. 31); used to assess malignant cell labeling; dataset reused (no public link provided).\"\n",
      "}\n",
      "{\n",
      "  \"data link\": \"Not available\",\n",
      "  \"repository\": \"Not available\",\n",
      "  \"accession\": \"P9962-23049B\",\n",
      "  \"platform\": \"10x Genomics Visium (spot-based spatial transcriptomics)\",\n",
      "  \"species\": \"Human\",\n",
      "  \"tissue\": \"Primary colorectal cancer\",\n",
      "  \"raw_data_available\": false,\n",
      "  \"available\": false,\n",
      "  \"original_data\": true,\n",
      "  \"description\": \"Spot-based spatial transcriptomics of a primary colorectal cancer region; matched bulk WGS generated; used for malignant spot labeling and subclone detection; generated in this study (no public link provided).\"\n",
      "}\n",
      "{\n",
      "  \"data link\": \"Not available\",\n",
      "  \"repository\": \"Not available\",\n",
      "  \"accession\": \"P9962-23050D\",\n",
      "  \"platform\": \"10x Genomics Visium (spot-based spatial transcriptomics)\",\n",
      "  \"species\": \"Human\",\n",
      "  \"tissue\": \"Primary colorectal cancer\",\n",
      "  \"raw_data_available\": false,\n",
      "  \"available\": false,\n",
      "  \"original_data\": true,\n",
      "  \"description\": \"Spot-based spatial transcriptomics of another nearby region from the same colorectal cancer patient; matched bulk WGS generated; used for malignant spot labeling and subclone detection; generated in this study (no public link provided).\"\n",
      "}\n",
      "{\n",
      "  \"data link\": \"Not available\",\n",
      "  \"repository\": \"Not available\",\n",
      "  \"accession\": \"SlideSeq-mouse (ref. 30, RNA)\",\n",
      "  \"platform\": \"Slide-seq (bead-based spatial transcriptomics, near single-cell resolution)\",\n",
      "  \"species\": \"Mouse\",\n",
      "  \"tissue\": \"Liver metastasis\",\n",
      "  \"raw_data_available\": false,\n",
      "  \"available\": false,\n",
      "  \"original_data\": false,\n",
      "  \"description\": \"Spatial transcriptomics (Slide-seq RNA) on a mouse liver metastasis with paired Slide-DNA-seq on serial sections; used to validate spatial subclone detection; dataset reused from ref. 30 (no public link provided).\"\n",
      "}\n",
      "{\n",
      "  \"data link\": \"Not available\",\n",
      "  \"repository\": \"Not available\",\n",
      "  \"accession\": \"SlideSeq-mouse (ref. 30, DNA)\",\n",
      "  \"platform\": \"Slide-DNA-seq (spatial DNA copy number profiling, near single-cell resolution)\",\n",
      "  \"species\": \"Mouse\",\n",
      "  \"tissue\": \"Liver metastasis\",\n",
      "  \"raw_data_available\": false,\n",
      "  \"available\": false,\n",
      "  \"original_data\": false,\n",
      "  \"description\": \"Spatial DNA sequencing paired with Slide-seq RNA on serial sections of a mouse liver metastasis; used as gold-standard validation for CNV and subclone structure (ref. 30); dataset reused (no public link provided).\"\n",
      "}\n",
      "{\n",
      "  \"data link\": \"Not available\",\n",
      "  \"repository\": \"Not available\",\n",
      "  \"accession\": \"SCC P6 (replicate 1 and 2)\",\n",
      "  \"platform\": \"Spatial transcriptomics (platform unspecified)\",\n",
      "  \"species\": \"Human\",\n",
      "  \"tissue\": \"Squamous cell carcinoma (two adjacent tissue slices)\",\n",
      "  \"raw_data_available\": false,\n",
      "  \"available\": false,\n",
      "  \"original_data\": true,\n",
      "  \"description\": \"Spatial transcriptomics on two adjacent sections (replicates) of an SCC sample with matched bulk WES; used for malignant spot labeling and reproducibility assessment; generated in this study (platform not specified; no public link provided).\"\n",
      "}\n",
      "{\n",
      "  \"data link\": \"https://www.10xgenomics.com/resources/datasets/human-breast-cancer-block-a-section-1-1-standard-1-0-0\",\n",
      "  \"repository\": \"10x Genomics\",\n",
      "  \"accession\": \"human-breast-cancer-block-a-section-1-1-standard-1-0-0\",\n",
      "  \"platform\": \"10x Genomics Visium (spot-based spatial transcriptomics)\",\n",
      "  \"species\": \"Human\",\n",
      "  \"tissue\": \"Breast cancer (invasive ductal carcinoma, Block A, Section 1)\",\n",
      "  \"raw_data_available\": true,\n",
      "  \"available\": true,\n",
      "  \"original_data\": false,\n",
      "  \"description\": \"Public Visium spot-based spatial transcriptomics dataset used to seed/traces subclones across adjacent sections; obtained from 10x Genomics.\"\n",
      "}\n",
      "{\n",
      "  \"data link\": \"https://www.10xgenomics.com/resources/datasets/human-breast-cancer-ductal-carcinoma-in-situ-invasive-carcinoma-ffpe-1-standard-1-3-0\",\n",
      "  \"repository\": \"10x Genomics\",\n",
      "  \"accession\": \"human-breast-cancer-ductal-carcinoma-in-situ-invasive-carcinoma-ffpe-1-standard-1-3-0\",\n",
      "  \"platform\": \"10x Genomics Visium FFPE (spot-based spatial transcriptomics)\",\n",
      "  \"species\": \"Human\",\n",
      "  \"tissue\": \"Breast ductal carcinoma in situ and invasive carcinoma (FFPE)\",\n",
      "  \"raw_data_available\": true,\n",
      "  \"available\": true,\n",
      "  \"original_data\": false,\n",
      "  \"description\": \"Public Visium FFPE spatial transcriptomics dataset used to identify spatially segregated subclones and perform differential expression; obtained from 10x Genomics.\"\n",
      "}\n",
      "{\n",
      "  \"data link\": \"Not available\",\n",
      "  \"repository\": \"Not available\",\n",
      "  \"accession\": \"Primary CRC (ref. 32)\",\n",
      "  \"platform\": \"Spatial transcriptomics (platform unspecified)\",\n",
      "  \"species\": \"Human\",\n",
      "  \"tissue\": \"Primary colorectal cancer\",\n",
      "  \"raw_data_available\": false,\n",
      "  \"available\": false,\n",
      "  \"original_data\": false,\n",
      "  \"description\": \"Spatial transcriptomics dataset from a primary CRC sample used for lineage tracing to a liver metastasis from the same patient; dataset reused from ref. 32 (no public link provided).\"\n",
      "}\n",
      "{\n",
      "  \"data link\": \"Not available\",\n",
      "  \"repository\": \"Not available\",\n",
      "  \"accession\": \"Liver metastasis (ref. 32)\",\n",
      "  \"platform\": \"Spatial transcriptomics (platform unspecified)\",\n",
      "  \"species\": \"Human\",\n",
      "  \"tissue\": \"Liver metastasis from colorectal cancer\",\n",
      "  \"raw_data_available\": false,\n",
      "  \"available\": false,\n",
      "  \"original_data\": false,\n",
      "  \"description\": \"Spatial transcriptomics dataset from a CRC liver metastasis used for lineage tracing with the matched primary tumor; dataset reused from ref. 32 (no public link provided).\"\n",
      "}\n",
      "{\n",
      "  \"data link\": \"Not available\",\n",
      "  \"repository\": \"Not available\",\n",
      "  \"accession\": \"SNU601 scATAC-seq\",\n",
      "  \"platform\": \"Single-cell ATAC-seq\",\n",
      "  \"species\": \"Human\",\n",
      "  \"tissue\": \"Gastric cancer cell line (SNU601)\",\n",
      "  \"raw_data_available\": false,\n",
      "  \"available\": false,\n",
      "  \"original_data\": false,\n",
      "  \"description\": \"Non-spatial scATAC-seq on the SNU601 gastric cancer cell line used to demonstrate allele-specific CNA-based subclone detection; matched scDNA-seq used as ground truth (ref. 9); dataset reused (no public link provided).\"\n",
      "}\n",
      "{\n",
      "  \"data link\": \"Not available\",\n",
      "  \"repository\": \"Not available\",\n",
      "  \"accession\": \"SNU601 scDNA-seq\",\n",
      "  \"platform\": \"Single-cell DNA-seq\",\n",
      "  \"species\": \"Human\",\n",
      "  \"tissue\": \"Gastric cancer cell line (SNU601)\",\n",
      "  \"raw_data_available\": false,\n",
      "  \"available\": false,\n",
      "  \"original_data\": false,\n",
      "  \"description\": \"Non-spatial single-cell DNA sequencing on SNU601 used as ground truth to define subclones and allele-specific CNAs for benchmarking scATAC-seq analysis (ref. 9); dataset reused (no public link provided).\"\n",
      "}\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "Processing paper 23 of 35, doi: 10.4239/wjd.v16.i9.107663\n",
      "Trying method 2: Unpaywall for DOI 10.4239/wjd.v16.i9.107663\n",
      "Failed method 2: Unpaywall for DOI 10.4239/wjd.v16.i9.107663\n",
      "Trying method 3: DOI content negotiation for DOI 10.4239/wjd.v16.i9.107663\n",
      "Failed method 3: DOI content negotiation for DOI 10.4239/wjd.v16.i9.107663\n",
      "Trying method 4: OpenAthens redirector for DOI 10.4239/wjd.v16.i9.107663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [02:14,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed method 4: OpenAthens redirector for DOI 10.4239/wjd.v16.i9.107663 (no PDF link found on page)\n",
      "All methods failed for DOI 10.4239/wjd.v16.i9.107663\n",
      "No full text found for DOI 10.4239/wjd.v16.i9.107663\n",
      "\n",
      "--------------------------------------------------------------------\n",
      "Processing paper 24 of 35, doi: 10.3389/fgene.2025.1615259\n",
      "Trying method 2: Unpaywall for DOI 10.3389/fgene.2025.1615259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [02:15,  5.89s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m oa_url \u001b[38;5;241m=\u001b[39m (one_paper\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moa_pdf_url\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m sess \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mSession()\n\u001b[0;32m---> 19\u001b[0m path, full_text \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_pdf_and_text_by_doi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mone_paper\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdoi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43msession\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# sess.close()\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_text:\n",
      "Cell \u001b[0;32mIn[8], line 276\u001b[0m, in \u001b[0;36mfetch_pdf_and_text_by_doi\u001b[0;34m(doi, save_dir, oa_pdf_url, openathens_prefix, session, timeout)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m pdf_url: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pdf_url:\n\u001b[0;32m--> 276\u001b[0m     pr \u001b[38;5;241m=\u001b[39m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _download_ok(pr):\n\u001b[1;32m    278\u001b[0m         pdf_bytes \u001b[38;5;241m=\u001b[39m pr\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/resnick/groups/mthomson/yunruilu/miniconda3/envs/Paper_Collection/lib/python3.9/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/resnick/groups/mthomson/yunruilu/miniconda3/envs/Paper_Collection/lib/python3.9/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/resnick/groups/mthomson/yunruilu/miniconda3/envs/Paper_Collection/lib/python3.9/site-packages/urllib3/connection.py:565\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    562\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/resnick/groups/mthomson/yunruilu/miniconda3/envs/Paper_Collection/lib/python3.9/http/client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1377\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/resnick/groups/mthomson/yunruilu/miniconda3/envs/Paper_Collection/lib/python3.9/http/client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/resnick/groups/mthomson/yunruilu/miniconda3/envs/Paper_Collection/lib/python3.9/http/client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 281\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/resnick/groups/mthomson/yunruilu/miniconda3/envs/Paper_Collection/lib/python3.9/socket.py:716\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 716\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/resnick/groups/mthomson/yunruilu/miniconda3/envs/Paper_Collection/lib/python3.9/ssl.py:1275\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1272\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1273\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1274\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/resnick/groups/mthomson/yunruilu/miniconda3/envs/Paper_Collection/lib/python3.9/ssl.py:1133\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = search_new_papers_bulk(since_days = since_days_input)\n",
    "print(f'Found {len(result)} new papers in the last {since_days_input} days since today: {datetime.date.today()}')\n",
    "\n",
    "for i, one_paper in tqdm(enumerate(result)):\n",
    "    if i < 21:\n",
    "        continue\n",
    "    print('\\n--------------------------------------------------------------------')\n",
    "    print(f\"Processing paper {i+1} of {len(result)}, doi: {one_paper['doi']}\")\n",
    "\n",
    "    # sess = requests.Session()\n",
    "\n",
    "    items = get_cited_papers(one_paper['paper_id'])\n",
    "    result[i]['reference'] = items\n",
    "    doi = one_paper['doi']\n",
    "    oa_url = (one_paper.get('oa_pdf_url') or None)\n",
    "\n",
    "    sess = requests.Session()\n",
    "\n",
    "    path, full_text = fetch_pdf_and_text_by_doi(doi = one_paper['doi'], \n",
    "                                                session = sess,\n",
    "                                                )\n",
    "    # sess.close()\n",
    "    if full_text:\n",
    "        data_info = extract_datasets_from_text(full_text = full_text)\n",
    "        if data_info:\n",
    "            if print_details_input:\n",
    "                print(f\"Successfully extracted datasets information\")\n",
    "                for ds in data_info:\n",
    "                    print(json.dumps(ds, indent=2))\n",
    "            result[i]['Datasets_info'] = data_info\n",
    "        else:\n",
    "            print(f\"Failed to extract datasets information for DOI {doi}\")\n",
    "            result[i]['Datasets_info'] = None\n",
    "    else:\n",
    "        print(f\"No full text found for DOI {doi}\")\n",
    "        result[i]['Datasets_info'] = None\n",
    "    # elif info[\"status\"] == \"auth_required\":\n",
    "    #     print(f\"Authentication required for DOI {doi}\")\n",
    "    #     # mark and skip or queue for manual login\n",
    "    #     result[i]['Datasets_info'] = None\n",
    "    # else:\n",
    "    #     result[i]['Datasets_info'] = None\n",
    "\n",
    "csv_path = \"/resnick/groups/mthomson/yunruilu/Github_repo/spatial-genomics-llm-collection/Papers.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "new_papers_df = pd.DataFrame(result)\n",
    "df_updated = pd.concat([df, new_papers_df], ignore_index=True)\n",
    "df_updated = df_updated.drop_duplicates(subset=['paper_id'], keep='first')\n",
    "\n",
    "df_updated.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Added {len(new_papers_df)} new papers to the CSV file\")\n",
    "print(f\"Total papers in CSV after deduplication: {len(df_updated)}\")\n",
    "# print(display(df_updated.head()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e73ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8eb7ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1774e4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f13b8de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d27aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed09e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paper_Collection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
